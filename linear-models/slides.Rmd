---
title: 'Normal linear models'
author: '<span style="font-size: 40px; font-face: bold">Jens Roeser</span>'
output: 
  ioslides_presentation:
    incremental: true
    transition: slower
    widescreen: true
    css: slides.css
    logo: ../gfx/ntu.png
bibliography      : ["../references.bib"]

---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, eval = TRUE, message = FALSE, comment=NA)
options("kableExtra.html.bsTable" = T, digits = 3)
options(pillar.print_min = 5, pillar.print_max = 6)
library(tidyverse)
library(knitr)
library(kableExtra)
library(patchwork)
theme_set(theme_bw(base_size = 18) +
            theme(legend.position = "top", 
                  legend.justification = "right",
                  panel.grid = element_blank()))
```


## Regression models {.smaller}

<div style="float: left; width: 50%;">
- Often introduced as fitting lines to points.
- Limited perspective that makes more complex regression models, like generalised linear models, hard to understand.
- Backbone of statistical modelling 
- For multiple / simple linear regressions, t-tests, ANOVAs, ANCOVAs, MANCOVAs, time series models
- Basis for path analysis, structural equation models, factor analysis
- Extension to generalized linear models: logistic regression for categorical data and count models such as poisson and negative binomial
- Generalised further to multilevel, aka hierarchical / mixed-effects modes
- Even nonlinear models: linear combination of nonlinear basis functions

</div>

<div style="float: right; width: 45%;">

```{r echo = F, out.width="100%"}
n <- 20
beta_0 <- 50
beta_1 <- 6
sigma <- 5
sim <- tibble(id = 1:n) %>%
    mutate(x = sample(0:5, size = n, replace = T),
           e = rnorm(n, mean = 0, sd = sigma)) %>% 
    mutate(y = beta_0 + beta_1 * x + e) %>%
  select(id, x, y)

ggplot(data =sim, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", se = F, colour = "red") +
  theme(axis.title = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank())
```

</div>

## Regression models

- A model of how the probability distribution of one variable, known as the *outcome variable*, varies as a function of other variables, known as the *explanatory* or *predictor variables*.
- The most basic type of regression models is the normal linear model.
- In normal linear models, we assume that the outcome variable is normally distributed and that its mean varies linearly with changes in a set of predictor variables.
- By understanding the normal linear model thoroughly, we can see how it can be extended to deal with data and problems beyond those that it is designed for.



## Normal linear model

<div style="float: left; width: 40%;">

$$
\begin{aligned}
y_i &\sim \mathcal{N}(\mu_i, \sigma^2), \text{ for } i \in 1\dots n\\
\end{aligned}
$$
</div>

<div style="float: right; width: 55%;">
- $y_i$ is the observed value of a univariate *outcome* variable.
- $N$ is a univariate normal distributed probability model with mean $\mu$ and variance $\sigma^2$ (or standard deviation $\sigma$)
- Each observation $y_i$ is a sample from such a normal distribution.
- Read "$\sim$" as "is proportional to" or "is a function of".
- Greek letters represent placeholders / variables for population parameter value; i.e. unknown values we want to estimate on the basis of assumptions represented as statistical model.
</div>



## Normal linear model

<div style="float: left; width: 40%;">

$$
\begin{aligned}
y_i &\sim \mathcal{N}(\mu_i, \sigma^2), \text{ for } i \in 1\dots n\\
\mu_i &= \beta_0 + \beta_1 \cdot x_{1i} \\
\end{aligned}
$$
</div>

<div style="float: right; width: 55%;">
- Value of $\mu_i$ is a deterministic (i.e. "$=$") linear function of the values of the predictor variables for each observation $i$.
- $\beta_0$ (intercept) and $\beta_1$ (slope) are unknown.
  - $\beta_1$ depends on $x_{1i}$.
  - $\beta_0$ is "constant" (i.e. does not depend on the value that a predictor $x$ takes on).
  - If $x_{1i}$ takes on 0 $\mu_i = \beta_0$.
  - If $x_{1i}$ takes on 1 $\mu_i = \beta_0 + \beta_1$.
  - Hence, $\beta_1$ is the difference between $x_{1i}=1$ and $x_{1i}=0$. 
</div>



## Normal linear model

<div style="float: left; width: 40%;">
$$
\begin{aligned}
y_i &\sim \mathcal{N}(\mu_i, \sigma^2), \text{ for } i \in 1\dots n\\
\mu_i &= \beta_0 + \beta_1 \cdot x_{1i} + \beta_2 \cdot x_{2i} \dots \beta_k \cdot x_{ki} \dots \beta_K \cdot x_{Ki}
\end{aligned}
$$

</div>


## Normal linear model


$$
\begin{aligned}
y_i &\sim \mathcal{N}(\mu_i, \sigma^2), \text{ for } i \in 1\dots n\\
\mu_i &= \beta_0 + \beta_1 \cdot x_{1i} + \beta_2 \cdot x_{2i} \dots \beta_k \cdot x_{ki} \dots \beta_K \cdot x_{Ki}\\
      & = \beta_0 + \sum_{k=1}^K \beta_k \cdot x_{ki}\\
\end{aligned}
$$



- Probabilistic model (i.e. "$\sim$") of $y_1 \dots y_n$ conditional on $\overrightarrow{x_1}\dots\overrightarrow{x_n}$, $\overrightarrow{\beta} = \beta_0, \beta_1\dots\beta_K$, and $\sigma$
- We use probabilistic models to predict / explain / understand the outcome variable. 
- $K$ values used to explain $y_i$. $\overrightarrow{x_i}$ are a set of $K$ *predictor* / *explanatory* variables. 
- Introducing $K$ in $x_{Ki}$ allows us to abbreviate the model equation.
- Which ones do we know and which ones must be inferred using statistical inference?  




## Normal linear model

<div style="float: left; width: 40%;">

$$
\begin{align}
y_i &\sim \mathcal{N}(\mu_i, \sigma^2)\\
\mu_i &= \beta_0 + \sum_{k=1}^K \beta_k \cdot x_{ki}\\
&\text{for } i \in 1\dots n
\end{align}
$$
</div>

<div style="float: right; width: 55%;">

- For every hypothetically possible value of the $K$ predictor variables, i.e. $\overrightarrow{x_{i'}}$,
there is a corresponding mean $\mu_{i'}$, i.e. $\mu_{i'} = \beta_0 + \sum_{k=1}^K \beta_k \cdot x_{ki'}$
- If we change $x_{ki'}$ by $\Delta_{k'}$, then $\mu_{i'}$ changes by $\beta_k\Delta_k$.

</div>

## Simulation of normal distributed data

- We can used `lm` to easily implement such a model.
- How do we know that `lm` returns accurate estimates for out model parameters?
- We can simulate data that exactly meet model assumptions (normal distribution, equal variance, independence etc.) with known model parameters.
- In simulations, we know that the model assumptions are met and we know the true parameter values, so we can evaluate whether the normal model does a sensible job in estimating those parameter values.


## Simulation of normal distributed data

<div style="float: left; width: 45%;">

- `rnorm` generates unimodal, symmetrically distributed values.
- `r` in `rnorm` = random; `norm` = normal

```{r}
# Generate data
x <- rnorm(n = 10, mean = 500, sd = 10)
```

</div>

<div style="float: right; width: 45%;">

```{r echo = F, out.width="100%"}
ggplot(data = NULL, aes(x = x)) +
  geom_histogram() +
  coord_cartesian(xlim = c(450, 550),
                  ylim = c(0, 100))
```
</div>


## Simulation of normal distributed data

<div style="float: left; width: 45%;">

- `rnorm` generates unimodal, symmetrically distributed values.
- `r` in `rnorm` = random; `norm` = normal

```{r}
# Generate data
x <- rnorm(n = 100, mean = 500, sd = 10)
```

</div>

<div style="float: right; width: 45%;">

```{r echo = F, out.width="100%"}
ggplot(data = NULL, aes(x = x)) +
  geom_histogram() +
  coord_cartesian(xlim = c(450, 550),
                  ylim = c(0, 100))
```
</div>


## Simulation of normal distributed data

<div style="float: left; width: 45%;">

- `rnorm` generates unimodal, symmetrically distributed values.
- `r` in `rnorm` = random; `norm` = normal

```{r}
# Generate data
x <- rnorm(n = 1000, mean = 500, sd = 10)
```

</div>

<div style="float: right; width: 45%;">

```{r echo = F, out.width="100%"}
ggplot(data = NULL, aes(x = x)) +
  geom_histogram() +
  coord_cartesian(xlim = c(450, 550),
                  ylim = c(0, 100))
```



## Simulation of normal distributed data

```{r}
# Set parameter values
n <- 1000
beta_0 <- 600 
beta_1 <- 30
sigma <- 50
```


```{r}
# Generate data
sim_data <- tibble(beta_0 = rnorm(n = n/2, mean = beta_0, sd = sigma),
                   beta_1 = rnorm(n = n/2, mean = beta_0 + beta_1, sd = sigma)) 
# Preview data
glimpse(sim_data)
```

## Simulation of normal distributed data

```{r}
# Set parameter values
n <- 1000
beta_0 <- 600 
beta_1 <- 30
sigma <- 50
```

```{r}
# Change data format
sim_data <- pivot_longer(sim_data, cols = c(beta_0, beta_1), names_to = "mu", values_to = "outcome")
# Preview data
glimpse(sim_data)
```

## Simulation of normal distributed data

```{r}
# Set parameter values
n <- 1000
beta_0 <- 600 
beta_1 <- 30
sigma <- 50
```

```{r}
# Preview data
glimpse(sim_data)
# Normal linear model of simulated data
model <- lm(outcome ~ mu, data = sim_data)
```


## Simulation of normal distributed data

<div style="float: left; width: 35%;">

```{r}
# Set parameter values
n <- 1000
beta_0 <- 600 
beta_1 <- 30
sigma <- 50
```

```{r}
# Model coefficients
coef(model)
```

```{r}
# Standard deviation
sigma(model)
```


</div>

<div style="float: right; width: 55%;">

```{r echo = F, out.width="100%"}
emmeans::emmeans(model, specs = "mu") %>% 
  as.tibble() %>% 
  ggplot(aes(x = mu, 
             y = emmean,
             ymin = lower.CL,
             ymax = upper.CL)) +
  geom_pointrange() +
  coord_cartesian(ylim = c(570, 660)) +
  scale_y_continuous(breaks = seq(500, 700, 10)) +
  labs(y = "Estimated means")
```

</div>

## Simulation: equality of variance

<div style="float: left; width: 35%;">

```{r}
# Set parameter values
n <- 1000
beta_0 <- 600 
beta_1 <- 30
sigma <- 150
```

```{r echo = F}
sim_data <- tibble(beta_0 = rnorm(n = n/2, mean = beta_0, sd = sigma),
                   beta_1 = rnorm(n = n/2, mean = beta_0 + beta_1, sd = sigma)) %>% 
  pivot_longer(cols = c(beta_0, beta_1), names_to = "mu", values_to = "outcome")
```

```{r echo = F}
# Normal linear model of simulated data
model <- lm(outcome ~ mu, data = sim_data)
```


```{r}
# Model coefficients
coef(model)
```


```{r}
# Standard deviation
sigma(model)
```

</div>

<div style="float: right; width: 55%;">

```{r echo = F, out.width="100%"}
emmeans::emmeans(model, specs = "mu") %>% 
  as.tibble() %>% 
  ggplot(aes(x = mu, 
             y = emmean,
             ymin = lower.CL,
             ymax = upper.CL)) +
  geom_pointrange() +
  coord_cartesian(ylim = c(570, 660)) +
  scale_y_continuous(breaks = seq(500, 700, 10)) +
  labs(y = "Estimated means")
```

</div>



```{r echo = F}
# Set parameter values
n <- 1000
beta_0 <- 600 
beta_1 <- 30
sigma <- 50

sim_data <- tibble(beta_0 = rnorm(n = n/2, mean = beta_0, sd = sigma),
                   beta_1 = rnorm(n = n/2, mean = beta_0 + beta_1, sd = sigma)) %>% 
  pivot_longer(cols = c(beta_0, beta_1), names_to = "mu", values_to = "outcome")

plot_sim <- ggplot(data = sim_data, aes(x = outcome, colour = mu, fill = mu)) +
  geom_histogram(position = "dodge") +
  scale_colour_viridis_d(end = .6) +
  scale_fill_viridis_d(end = .6) 
```


## Simulation: exercise

- Work through the scripts 
   - `exercises/normal_model_simulation_1.R`
   - `exercises/normal_model_simulation_2.R`
- You will need to complete the scripts.
- Then observe how changing the number of observations affects the estimates.

## Real data {.smaller}

<div style="float: left; width: 45%;">

- In real life we don't know the true parameter values as we do in simulations.
- We can use linear models to estimate parameter values from the data.
- Maximum likelihood estimation: for which set of parameter values are the data most likely.
- We also don't know what underlying process generates our real data.
- For the simulated data we know that the process is a normal distribution because `rnorm` samples normally distributed data.

</div>

<div style="float: right; width: 45%;">

```{r echo = F, out.width="100%", fig.height=7}
plot_sim / plot_spacer()
```

</div>


## Real data {.smaller}

<div style="float: left; width: 50%;">

- Sometimes even simulated data don't appear normal distribution because the sample is too small.

```{r}
blomkvist <- read_csv("../data/blomkvist.csv") %>% 
  select(id, sex, rt = rt_hand_d)
```

```{r echo = F}
glimpse(blomkvist, width = 50)
```

- Observed data are by-participant means, meaning the central limit theorem applies.
- Yet, reaction times are notoriously skewed [cause they have lower but no upper bound; see @baa08book].



</div>

<div style="float: right; width: 45%;">


```{r echo = F}
plot_blomkvist <- ggplot(data = blomkvist, aes(x = rt, colour = sex, fill = sex)) +
  geom_histogram(position = "dodge") +
  coord_cartesian(xlim = c(280, 1600),
                  ylim = c(0, 25)) +
  scale_x_continuous(breaks = seq(100, 2000, 200)) +
  scale_colour_viridis_d(end = .6) +
  scale_fill_viridis_d(end = .6) 

```

```{r echo = F, out.width="100%", fig.height=7}
plot_sim / plot_blomkvist
```

</div>



## Real data {.smaller}

<div style="float: left; width: 50%;">

Sometimes even simulated data don't appear normal distribution because the sample is too small.

```{r}
blomkvist <- read_csv("../data/blomkvist.csv") %>% 
  select(id, sex, rt = rt_hand_d) %>% 
  mutate(log_rt = log(rt))
```

```{r echo = F}
glimpse(blomkvist, width = 50)
```

- Observed data are by-participant means, meaning the central limit theorem applies.
- Yet, reaction times are notoriously skewed [cause they have lower but no upper bound; see @baa08book].

</div>

<div style="float: right; width: 45%;">



```{r echo = F}
log_blomkvist <- ggplot(data = blomkvist, aes(x = log_rt, colour = sex, fill = sex)) +
  geom_histogram(position = "dodge") +
  scale_x_continuous(breaks = seq(5, 8, .25)) +
  scale_colour_viridis_d(end = .6) +
  scale_fill_viridis_d(end = .6)

```

```{r echo = F, out.width="100%", fig.height=7}
plot_blomkvist / log_blomkvist + plot_layout(guides = "collect")
```

</div>


## Real data {.smaller}

<div style="float: left; width: 50%;">
- Data are unimodal, roughly normal distributed (there is some positive skew).
- Variance is roughly the same in both groups.
- There seem to be more longer rts for males (?).
- We assume that the $\text{log}(rt)$ that we denote $y_1, y_2\dots y_n$ are sampled from a normal distribution with a fixed and unknown mean $\mu$ ans standard deviation $\sigma$.


$$
y_i \sim \mathcal{N}(\mu_i, \sigma^2), \text{ for } i \in 1\dots n.
$$



</div>

<div style="float: right; width: 45%;">

```{r echo = F, out.width="100%", fig.height=7}
log_blomkvist +
  facet_wrap(~sex, nrow = 2) +
  theme(strip.background = element_blank(),
        strip.text = element_blank())
```

</div>


## Real data {.smaller}

<div style="float: left; width: 50%;">

Predictor variable sex is denoted as $x_1, x_2, \dots x_n$.


$$
\mu_i = \beta_0 + \beta_1 \cdot x_i
$$

where $x_i$ takes on $0$ for females and $1$ for males (because "f" is before "m" in the alphabet).


$$
x_i = \left\{ 
  \begin{array}{ll}
  0, \text{ if sex}_i = \texttt{female}\\
  1, \text{ if sex}_i = \texttt{male}\\
  \end{array}
\right.
$$


For females $\mu_i = \beta_0 + \beta_1 \cdot 0 = \beta_0$ and for males $\mu_i = \beta_0 + \beta_1 \cdot 1 = \beta_0 + \beta_1$, so $\beta_1$ gives the difference in the averages of the distribution of rts.



</div>

<div style="float: right; width: 45%;">

```{r echo = F, out.width="100%", fig.height=7}
log_blomkvist +
  facet_wrap(~sex, nrow = 2) +
  theme(strip.background = element_blank(),
        strip.text = element_blank())
```

</div>

## Real data: exercise


- Complete exercise scripts
  - `exercises/normal_model_blomkvist_1.R`
  - `exercises/normal_model_blomkvist_2.R`
- Make sure you understand how the model coefficients relate to the visualisation.



## Compare simulated and real data


<div style="float: left; width: 45%;">

```{r eval = F}
# Set parameter values
n <- 100
beta_0 <- 600 
beta_1 <- 30
sigma <- 50
```

```{r eval = F, echo = F}
# Generate data
sim_data <- tibble(beta_0 = rnorm(n = n/2, mean = beta_0, sd = sigma),
                   beta_1 = rnorm(n = n/2, mean = beta_0 + beta_1, sd = sigma)) 

```

```{r eval = F}
# Specify model
model <- lm(outcome ~ mu, data = sim_data)
```

```{r}
# Model coefficients
coef(model)
```


</div>

<div style="float: right; width: 45%;">

```{r}
# Load and transform data
blomkvist <- read_csv("../data/blomkvist.csv") %>% 
  select(id, sex, rt = rt_hand_d) %>% 
  mutate(log_rt = log(rt))
```

```{r}
# Specify model
model <- lm(log_rt ~ sex, data = blomkvist)
```

```{r}
# Model coefficients
coef(model)
```


</div>

## Interpreting coeffcients {.smaller}


<div style="float: left; width: 47%;">

Log transformation: slope is no longer additive (linear scale) but multiplicative (exponential scale)

```{r}
# Specify model
model <- lm(log_rt ~ sex, data = blomkvist)
# Model coefficients
(coefs <- as.vector(coef(model)))
```

- I.e. log rt difference isn't $\beta_0 + \beta_1$ but $\beta_0 \cdot \beta_1$; log rt is $\beta_1$ times shorter / longer.
- Intercept $\beta_0$ is the average for females and slope $\beta_1$ is difference for males.


</div>

<div style="float: right; width: 47%;">


```{r}
# Converting coefficients to msecs
intercept <- coefs[1]
slope <- coefs[2]
```

Exponentional function `exp` can be used to un-log coefficients.

```{r}
exp(intercept) # female group in msecs
```

```{r}
exp(intercept + slope) # male group in msecs
```


```{r}
exp(intercept + slope) - exp(intercept) 
```



</div>




## Interpreting coeffcients {.smaller}


<div style="float: left; width: 47%;">

According to the model, for any given sex $x'$, the corresponding distribution of rts is normally distributed with a mean $\mu' = \beta_0 + \beta_1 \cdot x'$ and standard deviation $\sigma$.

```{r}
coef(model)
```

```{r}
exp(intercept) # female group in msecs
```



</div>

<div style="float: right; width: 47%;">

- If sex changes by $\Delta_x$, the mean of the corresponding normal distribution over rts changes by
exactly $\beta_1\Delta_x$.
- This fact entails that if sex changes by exactly $\Delta_x=1$, the mean of the corresponding normal distribution over rts changes by exactly $\beta_1$.


```{r}
exp(intercept + slope) - exp(intercept) 
```

Complete `exercises/interpreting_coefficients_1.R`



</div>




## Contrast coding {.smaller}

Change contrast coding so intercept represents population average and slope shows difference between groups.


<div style="float: left; width: 35%;">

```{r echo = F}
# Coefficients from earlier
#exp(intercept); exp(intercept + slope) - exp(intercept)
```

```{r}
coef(model)
```



```{r echo=F}
blomkvist <- mutate(blomkvist, across(sex, factor))
```


```{r}
# Default: treatment contrast
contrasts(blomkvist$sex)
```


</div>

<div style="float: right; width: 56%;">

```{r}
# Sum contrast
contrasts(blomkvist$sex) <- c(-.5, .5)
colnames(contrasts(blomkvist$sex)) <- ": female vs male"
```

```{r echo =F}
contrasts(blomkvist$sex)
```


```{r}
# Re-fit model with sum contrast
model <- lm(log_rt ~ sex, data = blomkvist)
coef(model)
```

```{r eval = F, echo = F}
coefs <- as.vector(coef(model))
intercept <- coefs[1]; slope <- coefs[2]

exp(intercept) # overall average
exp(intercept + slope) - exp(intercept) # difference 

```

Complete `exercises/interpreting_coefficients_2.R`


</div>

## Normal linear model of rts by sex and age {.smaller}

<div style="float: left; width: 45%;">

- We may also model how the distribution of rts varies as either, or both, sex and age.
- In this figure, we see the density of rts for the different sexes, and the different terciles of age.
- Denoting sexes by $x_{11},x_{12}\dots x_{1i}\dots x_{1n}$ and age by $x_{21},x_{22}\dots x_{2i}\dots x_{2n}$ the model is now

$$
y_i \sim \mathcal{N}(\mu_i, \sigma^2)\\
\mu_i = \beta_0 + \beta_1 \cdot x_{1i} + \beta_2 \cdot x_{2i}.
$$


</div>

<div style="float: right; width: 45%;">

```{r echo = F, out.width="100%", fig.height=7}
# Load and transform data
blomkvist <- read_csv("../data/blomkvist.csv") %>% 
  select(id, sex, age, rt = rt_hand_d) %>% 
  mutate(log_rt = log(rt),
         age = cut(age, breaks = 3, labels = 1:3))

ggplot(blomkvist, aes(x = log_rt, colour = sex, fill = sex)) +
  geom_density(alpha = .5, show.legend = F) +
  facet_grid(age ~ sex, labeller = label_both) +
  scale_colour_viridis_d(end = .6) +
  scale_fill_viridis_d(end = .6)
```

</div>


## Normal linear model of rts by sex and age {.smaller}

<div style="float: left; width: 45%;">

- For any given combination of sex and age, we have a normal distribution over rts.
- If the sex variable changes by one unit, when age is held constant, then the average value of the corresponding distribution of rts changes by $\beta_1$.
- Conversely, if the age variable changes by one unit, when sex is held constant, then the average value of the corresponding distribution of rts changes by $\beta_2$.


$$
y_i \sim \mathcal{N}(\mu_i, \sigma^2)\\
\mu_i = \beta_0 + \beta_1 \cdot x_{1i} + \beta_2 \cdot x_{2i}.
$$


</div>

<div style="float: right; width: 45%;">

```{r echo = F, out.width="100%", fig.height=7}
# Load and transform data
blomkvist <- read_csv("../data/blomkvist.csv") %>% 
  select(id, sex, age, rt = rt_hand_d) %>% 
  mutate(log_rt = log(rt),
         age = cut(age, breaks = 3, labels = 1:3))

ggplot(blomkvist, aes(x = log_rt, colour = sex, fill = sex)) +
  geom_density(alpha = .5, show.legend = F) +
  facet_grid(age ~ sex, labeller = label_both) +
  scale_colour_viridis_d(end = .6) +
  scale_fill_viridis_d(end = .6)
```

</div>


## Model fitting {.smaller}

If we assume the model


$$
y_i \sim \mathcal{N}(\mu_i, \sigma^2) \text{, for } i \in 1\dots n\\ 
\mu_i = \beta_0 + \sum_{k=1}^K \beta_k \cdot x_{ki}
$$

we face the problem of inferring the values of $K+2$ unknowns $\beta_0, \beta_1\dots\beta_K$, and $\sigma$. 

- In general in statistics, maximum likelihood estimation is a method by which we estimate the values of the unknown variables in a statistical model.
- In the case of normal linear model, the maximum likelihood estimators of the $K + 2$ unknowns, which we can denote by

$$
\hat\beta_0,\hat\beta_1\dots\hat\beta_K,\hat\sigma,
$$
are the set of values $K+2$ unknown variables that make the observed data most probable.

## Model fitting {.smaller}

<div style="float: left; width: 40%;">

The maximum likelihood values of the $K+1$ coefficients, $\hat\beta_0,\hat\beta_1\dots\hat\beta_K$, are those values of the $\beta_0,\beta_1\dots\beta_K$ variables that minimize the *residual sum of squares*

$$
\text{RSS} = \sum_{i=1}^n \mid y_i - \mu_i\mid^2,
$$

where $\mu_i$ is defined above.

</div>

<div style="float: right; width: 50%;">

```{r}
# Define population parameter
mu <- 100
# Generate random data from normal distribution
y <- rnorm(10, mean = mu, sd = 10)
# Propose values for population parameter
mu_grid <- seq(0, 500, 10)
# Check which parameter value returns lowest RSS
rss <- c()
for(mu in mu_grid){
  # calculate rss for mu
  rss_mu <- sum((y - mu) ^ 2)
  rss <- c(rss, rss_mu)
}
# Look for the smallest rss value
idx <- which.min(rss)
# Find corresponding mu value
mu_grid[idx]
```

See script `exercises/mle.R` for example.

</div>

## Fitting a normal linear model using `lm` {.smaller}

- `lm` is using maximum likelihood estimation and can be used for larger sets of parameters.
- Let's continue with the @blomkvist2017reference rt data.
- For simplicity of explanation we will use the untransformed rts.


```{r echo = F}
blomkvist <- read_csv("../data/blomkvist.csv") %>% 
  select(id, sex, age, rt = rt_hand_d) %>% 
  mutate(log_rt = log(rt)) %>% 
  drop_na()
```


```{r}
model <- lm(rt ~ sex + age, data = blomkvist)
```

- Maximum likelihood estimators of $\beta_0$ (intercept) and $\beta_1$ (coefficient of sex) and $\beta_2$ (coefficent of age) are as follow:

```{r}
(coefs <- coef(model))
```




## Fitting a normal linear model using `lm` {.smaller}


<div style="float: left; width: 42%;">

Once we have $\hat\mu$, then the maximum likelihood estimate of $\sigma$, denoted as $\hat\sigma$ is:

$$
\hat\sigma = \sqrt{\frac{1}{n-K-1}\sum_{i=1}^n\mid y_i-\hat\mu_i\mid^2}.
$$


Maximum likelihood estimate of $\sigma$ is

```{r}
sigma(model)
```

</div>

<div style="float: right; width: 52%;">

```{r}
mutate(blomkvist, 
       sex = recode(sex, female = 0, male = 1),
       mu = coefs[1] + coefs[2]*sex + coefs[3]*age,
       squared_diffs = (rt - mu)^2) %>% 
summarise(sum_of_squres = sum(squared_diffs),
          K = 2,
          N = n(),
          sigma_2 = 1 / (N-K-1) * sum_of_squres,
          sigma = sqrt(sigma_2))
```
Complete `exercises/calculating_sigma.R`



</div>


## Hypothesis testing and confidence intervals

If $\beta_k$ is the *hypothesized* true value of the coefficient, then

$$
\frac{\hat\beta_k - \beta_k}{\hat{\text{se}}_k} \sim t_{n-K-1}, 
$$
which tells us that if the true value of the coefficient was some hypothesized value $\beta_k$, then we expect the *t*-statistic to have a certain range of values with high probability.

## Hypothesis testing and confidence intervals {.smaller}

We will test a hypothesis about the coefficient for sex in the model above.

<div style="float: left; width: 47%;">

```{r}
# Extract the coefficients table from summary
coefs <- summary(model)$coefficients
# MLE for sex
(beta_sex <- coefs['sexmale', 'Estimate'])
```

```{r}
# Standard error
(se_sex <- coefs['sexmale', 'Std. Error'])
```

</div>

<div style="float: right; width: 47%;">

If we hypothesize the true value of the coefficient is exactly 1.0,
then our *t*-statistic is

```{r}
(t <- (beta_sex - 1.0)/se_sex)
```

</div>


$$
\frac{\hat\beta_k - \beta_k}{\hat{\text{se}}_k} = \frac{-58.9 - 1.0}{18.5} = -3.24
$$

## *p*-values {.smaller}

- The *p*-value for the hypothesis is the probability of getting a value *as or more extreme* than the absolute value of *t*-statistic in the *t*-distribution.
- This is the area under the curve in the *t*-distribution that is as or
more extreme than the absolute value of *t*-statistic.
- It tells us how far into the tails of the distribution the *t*-statistic is.

```{r}
n <- nrow(blomkvist)
K <- 2
pt(abs(t), df = n - K - 1, lower.tail = FALSE) * 2
```

## Hypothesis testing and confidence intervals {.smaller}

*Null hypothesis significance testing*: we hypothesize the true value of the coefficient is exactly 0.0, then our *t*-statistic is

$$
\frac{\hat\beta_k - \beta_k}{\hat{\text{se}}_k} = \frac{-58.9 - 0.0}{18.5} = \frac{-58.9}{18.5} = -3.19
$$

```{r}
(t <- (beta_sex - 0.0)/se_sex)
```

```{r}
n <- nrow(blomkvist)
K <- 2
pt(abs(t), df = n - K - 1, lower.tail = FALSE) * 2
```


## *p*-values {.smaller}

- Work through script `exercise/t_values.R` 
- Calculate *t* and *p*-value for $\hat\beta_\text{age}$ testing the null hypothesis that the true value $\beta_\text{age}$ is 0.0.
- Verify your calculation using

```{r}
summary(model)$coefficients
```




## *t*-distribution {.smaller}


```{r echo = F,  out.width="80%", fig.height=4}
t <- abs(beta_sex / se_sex)
n <- nrow(blomkvist)
K <- 1
df <- n - K - 1
p_value <- pt(t, df = df, lower.tail = FALSE) * 2

# Hypothetical t values
t_grid <- seq(0, 10, .1) 
p_values <- pt(t_grid, df = df, lower.tail = FALSE) * 2

p <- ggplot(data = NULL, aes(x = t_grid, y = p_values)) +
  geom_line() +
  geom_hline(yintercept = 0.05, linetype = 'dotted') +
  scale_x_continuous(breaks = seq(0, 20, 1)) +
  labs(x = 'Theoretical t-value',
       y = 'p-value',
       subtitle = bquote("t distribution for"~"Df" == .(df)));p 

```


## *t*-distribution {.smaller}

```{r echo = F,  out.width="80%", fig.height=4}
label <- deparse(bquote(paste("t" == .(round(t, 2)))))

p2 <- p + geom_segment(aes(x = t+1, y = .2, xend = t, yend = 0.02),
                  arrow = arrow(length = unit(0.35, "cm")),
               colour = "red") +
  geom_label(aes(x = t + 1, y = 0.3, label = label), parse = T); p2
```

## *t*-distribution {.smaller}

```{r echo = F,  out.width="80%", fig.height=4}
label_2 <- deparse(bquote(paste("p" == .(round(p_value, 3)))))
p2 + geom_hline(yintercept = p_value, colour = "red", linetype = "dashed") +
  geom_label(aes(x = .65, y = 0.05, label = label_2), parse = T) 

```


## *t*-distribution {.smaller}

```{r echo = F,  out.width="80%", fig.height=4}
label_2 <- deparse(bquote(paste("p" == .(round(p_value, 3)))))
p2 + geom_hline(yintercept = p_value, colour = "red", linetype = "dashed") +
  geom_label(aes(x = .65, y = 0.05, label = label_2), parse = T) +
  annotate("label", x = 8, y = .9, label = "Work through script\n exercise/t_distribution.R")

```




## *t*-distribution {.smaller}


```{r echo = F, out.width="80%", fig.height=4}
t_grid <- seq(0, 15, .1) 
df <- c(1, 2, 5, 8, 10, 100, 1000, 10000)

data_df <- map_dfr(df, ~pt(t_grid, df = .x, lower.tail = F) %>% tibble(t_grid = t_grid, p = . * 2) %>%  
    mutate(df = .x)) 

ggplot(data = data_df, aes(x = t_grid, y = p, colour = factor(df))) +
  geom_line() +
  labs(x = "Theoretical t-value",
       y = "p-value",
       subtitle = "t-distribution",
       colour = bquote("Df")) +
  scale_colour_viridis_d() +
  scale_x_continuous(breaks = seq(0, 20, 1)) +
  geom_hline(yintercept = 0.05, linetype = "dotted") +
  theme(legend.position = c(.925,.55),
        legend.title = element_text(size = 10),
        legend.text = element_text(size = 10))


```


## Confidence intervals {.smaller}

- The set of all hypotheses that we do not rule out at the $\alpha < 0.05$ level of significance is known as the 0.95 (or 95%) confidence interval.
- Likewise, the set of all hypotheses that we do not rule out at the $\alpha < 0.01$ level of significance is known as the 0.99 confidence interval, and so on.
- In a normal linear model, the formula for the confidence interval on coefficient $k$ is

$$
\hat\beta_k \pm \tau_{(1-\epsilon,\nu)} \cdot \hat{\text{se}}_k,
$$
where $\tau_{(1-\epsilon,\nu)}$ is value in a *t*-distribution with $\nu$ degrees of freedom below which is $1-\epsilon$ of the area under the curve.

- $\tau$: how many standard errors above and below ('$\pm$') $\hat\beta_k$ are the lower and upper bound of the confidence interval.




## Confidence intervals {.smaller}

<div style="float: left; width: 40%;">

- We can calculate $\tau$ using `qt`

```{r}
(tau <- qt(.975, df = n - K - 1))
```

We then obtain the confidence intervals as follows

```{r}
beta_sex + c(-1, 1) * se_sex * tau
```

</div>

<div style="float: right; width: 50%;">

```{r}
coef(model) 
```

Verification:

```{r}
confint(model, parm = 'sexmale',  level = .95)
```

Open and complete script `exercises/confidence_interval.R`

</div>


## Confidence intervals: a simulation {.smaller}

<div style="float: left; width: 50%;">

```{r echo = F}
set.seed(324)
```


```{r}
# Set simulation parameters
mu <- 600 # True parameter value
n <- 1000 # Number of observations per experiment
n_sims <- 10 # Number of hypothetical experiments
# Empty data frame for results
sims <- tibble()
# Run n_sims simulations
for(i in 1:n_sims){
  # Simulate normal distributed data
  y <- rnorm(n, mean = mu, sd = 100)
  # Get MLEs
  m <- lm(y ~ 1)
  # Extract CI
  cis <- confint(m) %>% as.vector()
  # Store results
  sims <- tibble(sim_id = i, 
                 lower = cis[1], 
                 upper = cis[2]) %>% 
    bind_rows(sims)
}
```

</div>

<div style="float: right; width: 45%;">


```{r echo = FALSE, out.width="100%", fig.height=7}
mutate(sims, 
       mu_in_ci = ifelse(mu < lower | mu > upper, "no", "yes")) %>% 
  ggplot(aes(x = sim_id, 
             ymin = lower, 
             ymax = upper, 
             colour = mu_in_ci)) +
  geom_errorbar(width = 0) +
  geom_hline(yintercept = mu, linetype = "dotted", colour = "blue") +
  scale_colour_manual(values = c("no" = "red", "yes" =" black")) +
  scale_x_continuous(breaks = seq(1, n_sims, 1)) +
  scale_y_continuous(limits = c(580, 620)) +
  labs(colour = bquote("95% CI includes"~mu*":"),
        caption = bquote(.(n_sims)~"simulations"),
       x = "Experiment id (simulation)",
       y = "Parameter estimate") +
  coord_flip()

```
</div>

## Confidence intervals: a simulation 

- Complete and run script 
  - `exercises/confidence_interval_simulation.R`
- Change the number of simulations to, one at a time, `n_sims = 20,50,100,500,1000`
- Notice how often the CI does not contain the true parameter value $\mu$.
- How many % of simulations contain $\mu$ (it's okay to guess)?

## Confidence intervals: a simulation {.smaller}

<div style="float: left; width: 45%;">

```{r echo = F}
mu <- 600 # True parameter value
n_sims <- 1000 # Number of hypothetical experiments
n <- 1000 # Number of observations per experiment
sims <- tibble()
for(i in 1:n_sims){
  y <- rnorm(n, mean = mu, sd = 100)
  m <- lm(y ~ 1)
  cis <- confint(m) %>% as.vector()
  sims <- tibble(sim_id = i, lower = cis[1], upper = cis[2]) %>% 
    bind_rows(sims)
}
```

```{r echo = FALSE, out.width="100%", fig.height=7}
sims <- mutate(sims, mu_in_ci = ifelse(mu < lower | mu > upper, "no", "yes"))
ggplot(sims, aes(x = sim_id, 
                 ymin = lower, 
                 ymax = upper, 
             colour = mu_in_ci)) +
  geom_errorbar(width = 0) +
  geom_hline(yintercept = mu, linetype = "dotted", colour = "blue") +
  scale_colour_manual(values = c("no" = "darkred", "yes" =" grey50")) +
  scale_y_continuous(limits = c(580, 620)) +
  labs(colour = bquote("95% CI includes"~mu*":"),
       caption = bquote(.(n_sims)~"simulations"),
       x = "Experiment id (simulation)",
       y = "Parameter estimate") +
  coord_flip()
```

</div>

<div style="float: right; width: 45%;">

```{r}
count(sims, mu_in_ci) %>% 
  mutate(prop = n / n_sims)
```

- When our number of experiments approaches $\infty$ and we calculate a 95% CI for each experiment, 95% of these CIs will contain $\mu$.
- Note though, 5% of the time, the confidence interval does not contain $\mu$!
- What does this mean for real (as opposed to simulated) experiments?


</div>



## Predictions {.smaller}

<div style="float: left; width: 40%;">

Using $\hat\beta_0,\hat\beta_1\dots\hat\beta_K$ and $\hat\sigma^2$, then for any new vector of predictor variables $\overrightarrow{x}_{i'}$, the corresponding $y_i$ is now

$$
y_{i'} \sim \mathcal{N}(\hat\mu_{i'}, \sigma^2)\\ 
\hat\mu_{i'} = \hat\beta_0 + \sum_{k=1}^K \hat\beta_k \cdot x_{i'k}
$$

```{r}
coef(model)
```


</div>

<div style="float: right; width: 50%;">


As $K$ is 2 we can rewrite, then simplify, and predict rts for females age 95.

$$
\begin{aligned}
\hat\mu_i &= \hat\beta_0 + \hat\beta_1 \cdot x_{i1} + \hat\beta_2 \cdot x_{i2}\\
&=338.64 + -58.83 \cdot x_{i1} + 5.75 \cdot x_{i2}\\
&=338.64 + -58.83 \cdot 0 + 5.75 \cdot 95\\
&=338.64 + 5.75 \cdot 95\\
&=885
\end{aligned}
$$

To calculate $\hat\mu_{i'}$ for any given new vector of predictor variables $\overrightarrow{x}_{i'}$ use `predict`.

```{r}
newdata <- tibble(sex = "female", age = 95)
predict(model, newdata = newdata) 
```



</div>

## Prediction confidence intervals {.smaller}

- We can calculate confidence intervals for the predicted values of $\mu_{i'}$.
- The confidence interval for $\mu_{i'}$ is as follows:

$$
\hat\mu_{i'} \pm \tau_{(1-\epsilon,\nu)} \cdot \hat{\text{se}}_{\mu_{i'}}
$$

where $\hat{\text{se}}_{\mu_{i'}}$ is the standard error term that corresponds to $\hat\mu_{i'}$.

- We can obtain the confidence intervals on $\hat\mu_{i'}$ by using the option `interval = 'confidence'` in the `predict` function.

```{r}
predict(model, interval = 'confidence', newdata = newdata)
```


## Prediction confidence intervals: exercise

- Work through scripts
  - `exercises/predictions.R`
  - `exercises/predictions_with_cis.R`


## Interactions and varying intercepts model {.smaller}

When we include `sex` as an explanatory variable in additional to a continuous predictor variable, the model is as follows. 

$$
y_i \sim \mathcal{N}(\mu_i, \sigma^2), \mu_i = \beta_0 + \beta_1 \cdot x_{1i} + \beta_2 \cdot x_{2i}, \text{ for } i \in 1\dots n.
$$

To implement this model using `lm` we would do the following.


```{r}
model_2 <- lm(log_rt ~ sex + age, data = blomkvist)
coef(model_2)
```

## Interactions and varying intercepts model {.smaller}

Given that $x_{2i}$ takes that value of 0 when the gender is female and 1 when gender is male, this model can be written as follows.


$$
y_i \sim \mathcal{N}(\mu_i, \sigma^2) \\
\mu_i =  \left\{ 
\begin{array}{ll}
\beta_0 + \beta_2 \cdot x_{2i}, \text{ if sex}_i = \texttt{female}\\
\beta_0 + \beta_1 \cdot x_{1i} + \beta_2 \cdot x_{2i}, \text{ if sex}_i = \texttt{male}
\end{array}
\right.
$$
This is a *varying intercepts* model.

## Interactions and varying intercepts model {.smaller}

```{r echo = F}
model <- lm(log_rt ~ age, data = blomkvist)

blomkvist$pred <- predict(model)
blomkvist$resid <- residuals(model)

plot <- ggplot(blomkvist, aes(x = age, y = pred) ) +
  stat_smooth(method = "lm", size = 2, colour = "darkred") +
  labs(y = "log rts", 
       subtitle = bquote("y"[i]~"~"~"N("*mu["i"]*","~sigma^2*"),"~mu["i"] == beta[0]~+~beta["1"]~"*"~"x"["age"])) +
  geom_point(aes(x = age, y = log(rt)), size = 1);plot

```


## Interactions and varying intercepts model {.smaller}

```{r echo = F}
model <- lm(log_rt ~ age + sex, data = blomkvist)

blomkvist$pred <- predict(model)
blomkvist$resid <- residuals(model)

plot <- ggplot(blomkvist, aes(x = age, y = pred, colour = sex) ) +
  stat_smooth(method = "lm", size = 2) +
  labs(y = "log rts", 
       subtitle = bquote("y"[i]~"~"~"N("*mu["i"]*","~sigma^2*"),"~mu["i"] == beta[0]~+~beta["1"]~"*"~"x"["age"]~+~beta["2"]~"*"~"x"["sex"])) +
  geom_point(aes(x = age, y = log(rt), colour = sex), size = 1) +
  scale_colour_viridis_d(end = .6) +
  theme(legend.position = c(.45,.85),
        legend.direction = "horizontal");plot

```

## Interactions and varying-slopes model {.smaller}


The varying-intercepts model is written as follows. For $i \in 1\dots n$

$$
\begin{aligned}
y_i &\sim N(\mu_i, \sigma^2)\\
\mu_i &= \beta_0 + \beta_1 \cdot x_{1i} + \beta_2 \cdot x_{2i}\\
\end{aligned}
$$

The following is a varying intercepts and varying slopes model:

$$
\begin{aligned}
y_i &\sim N(\mu_i, \sigma^2)\\ 
\mu_i &= \beta_0 + \beta_1 \cdot x_{1i} + \beta_2 \cdot x_{2i} + \underbrace{\beta_3 \cdot x_{1i} \cdot x_{2i}}_\text{interaction}
\end{aligned}
$$
We effectively have a third predictor $x_{1i} \cdot x_{2i}$ that is the product of $x_{1i}$ and $x_{2i}$.


## Interactions and varying-slopes model {.smaller}

Using `sex` and `age` as predictors, we can do the following to perform a varying-intercepts and varying-slopes model:

```{r}
model_3 <- lm(log_rt ~ sex + age + sex:age, data = blomkvist)
```

Or slightly shorter:

```{r}
model_3 <- lm(log_rt ~ sex * age, data = blomkvist)
```


## Interactions and varying-slopes model {.smaller}


<div style="float: left; width: 45%;">

```{r echo = F, out.width="100%", fig.height=7}
model <- lm(log_rt ~ age + sex, data = blomkvist)

blomkvist$pred <- predict(model)
blomkvist$resid <- residuals(model)

plot <- ggplot(blomkvist, aes(x = age, y = pred, colour = sex) ) +
  stat_smooth(method = "lm", size = 2) +
  labs(y = "log rts", 
       subtitle = bquote("y"[i]~"~"~"N("*mu["i"]*","~sigma^2*"),"~mu["i"] == beta[0]~+~beta["1"]~"*"~"x"["age"]~+~beta["2"]~"*"~"x"["sex"])) +
  geom_point(aes(x = age, y = log(rt), colour = sex), size = 1) +
  scale_colour_viridis_d(end = .6) +
  theme(legend.position = c(.45,.9),
        legend.direction = "horizontal",
        plot.subtitle = element_text(size = 14));plot

```

</div>

<div style="float: right; width: 45%;">

```{r echo = F, out.width="100%", fig.height=7}
model <- lm(log_rt ~ age * sex, data = blomkvist)

blomkvist$pred <- predict(model)
blomkvist$resid <- residuals(model)

plot <- ggplot(blomkvist, aes(x = age, y = pred, colour = sex) ) +
  stat_smooth(method = "lm", size = 2) +
  labs(y = "log rts", 
       subtitle = bquote("y"[i]~"~"~"N("*mu["i"]*","~sigma^2*"),"~mu["i"] == beta[0]~+~beta["1"]~"*"~"x"["age"]~+~beta["2"]~"*"~"x"["sex"]~+~beta["3"]~"*"~"x"["sex"]~"*"~"x"["age"])) +
  geom_point(aes(x = age, y = log(rt), colour = sex), size = 1) +
  scale_colour_viridis_d(end = .6) +
  theme(legend.position = c(.45,.9),
        legend.direction = "horizontal",
        plot.subtitle = element_text(size = 14));plot

```

</div>


## Interactions and varying-slopes model {.smaller}


<div style="float: left; width: 50%;">

For simplicity, lets start with untransformed rts.

```{r}
model_3 <- lm(rt ~ sex * age, data = blomkvist)
```

From the varying-intercepts and varying-slopes model, the intercept terms for `female` and `male` are

```{r}
(betas <- coef(model_3))
```


```{r echo = F}
betas <- as.vector(coef(model_3))
```


```{r}
# females
betas[1]
```


</div>

<div style="float: right; width: 42%;">




```{r}
# males
betas[1] + betas[2]
```


The slope terms are as follows:

```{r}
# females
betas[3]
```


```{r}
# males
betas[3] + betas[4]
```

</div>

## Interactions and varying-slopes model {.smaller}

```{r}
model_3 <- lm(log_rt ~ sex * age, data = blomkvist)
```

From the varying-intercepts and varying-slopes model, the intercept terms for `female` and `male` are

<div style="float: left; width: 45%;">

```{r}
(betas <- coef(model_3))
```

```{r echo = F}
betas <- as.vector(coef(model_3))
```


```{r}
# females
exp(betas[1]) 
```


```{r}
# males
exp(betas[1] + betas[2])
```

</div>

<div style="float: right; width: 45%;">

The slope terms are as follows:

```{r}
# females
exp(betas[1] + betas[3]) - exp(betas[1])
```


```{r}
# males
exp(betas[1] + betas[3] + betas[4]) -
  exp(betas[1])
```


</div>


## Interactions and varying-slopes model: exercise

- Work through scripts
  - `exercises/varying_intercepts_model.R`
  - `exercises/varying_slopes_model.R`


## Polychotomous predictors {.smaller}

We can also use categorical predictor variables that have more than two levels (i.e. dichotomous like `sex`), which can be referred to as polychotomous predictor variables.


<div style="float: left; width: 55%;">


```{r}
# Load and pre-process data
blomkvist <- read_csv("../data/blomkvist.csv") %>% 
  select(id, sex, age, rt = rt_hand_d, smoker) %>% 
  mutate(log_rt = log(rt)) %>% 
  drop_na()
```

</div>

<div style="float: right; width: 35%;">

```{r}
# Categories of smoker
unique(blomkvist$smoker)
```

</div> 


<div style="float: left; width: 100%;">

Using `smoker` as out single categorical predictor variable, the linear model would be as follows.

$$
y_i \sim N(\mu_i, \sigma^2), \mu_i = \beta_0 + \beta_1 \cdot x_{1i} + \beta_2 \cdot x_{2i}, \text{ for } i \in 1\dots n.
$$

where $x_{1i}$, $x_{2i}$ are as follows.

$$
x_{1i}, x_{2i}
 =  \left\{ 
\begin{array}{ll}
0,0 \text{ if smoker}_i = \texttt{former}\\
1,0 \text{ if smoker}_i = \texttt{no}\\
0,1 \text{ if smoker}_i = \texttt{yes}\\
\end{array}
\right.
$$

</div> 

## Polychotomous predictors {.smaller}


Using `lm`, we would simply do as follows.

```{r}
model_smoker <- lm(log_rt ~ smoker, data = blomkvist)
(betas <- coef(model_smoker))
```


```{r echo = F}
betas <- as.vector(coef(model_smoker))
```


The intercept term is the predicted average of the distribution of rt when `smoker` is `former`.

```{r}
exp(betas[1])
```



## Polychotomous predictors {.smaller}


Using `lm`, we would simply do as follows.

```{r}
model_smoker <- lm(log_rt ~ smoker, data = blomkvist)
(betas <- coef(model_smoker))
```



```{r echo = F}
betas <- as.vector(coef(model_smoker))
```


The predicted mean of the weight distribution for `no`

$$
\hat\beta_0 + \hat\beta_1 \cdot 1 + \hat\beta_2 \cdot 0 
$$




```{r}
exp(betas[1] + betas[2] * 1 + betas[3] * 0)
```



## Polychotomous predictors {.smaller}


Using `lm`, we would simply do as follows.

```{r}
model_smoker <- lm(log_rt ~ smoker, data = blomkvist)
(betas <- coef(model_smoker))
```

```{r echo = F}
betas <- as.vector(coef(model_smoker))
```

When `smoker` is `yes`, the predicted mean of the rt distribution for smokers is 


$$
\hat\beta_0 + \hat\beta_1 \cdot 0 + \hat\beta_2 \cdot 1 
$$

```{r }
exp(betas[1] + betas[2] * 0 + betas[3] * 1)
```

Exercise: work through script `exercises/polychotomous_predictor.R`

</div>

## Recap

- Normal model assumes that data come from a single process that generates normal distributed data.
- Log-normal transformation is often used to correct positive skew.
- MLE provides estimates for unknown population parameters.
- We can specify models with continues and categorical predictors and combinations of them.
- Model specifications using varying intercepts and varying slopes.
- Read lecture notes on NOW [@andrews2021doing].
- Model comparisons: has age a stronger effect for females than for males?

## Homework

- We will start preparing your report for the formative assessment.
- Review the [details for the formative assessment.](https://now.ntu.ac.uk/d2l/le/content/887088/viewContent/9653015/View)

> *Using a data-set of your own choice, perform and report a linear regression analysis to address a set of theoretical questions of your choice that relate to your chosen data-set*

- Familiarize yourself with [the demo](https://now.ntu.ac.uk/d2l/le/content/887088/viewContent/9653011/View) for the formative assessment.
- Find a suitable data set you can use for the formative assessment.


## References {.smaller}

<style>
slides > slide { overflow: scroll; }
slides > slide:not(.nobackground):after {
  content: '';
}
</style>


