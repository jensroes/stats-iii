---
title: 'Normal linear models'
author: '<span style="font-size: 40px; font-face: bold">Jens Roeser</span>'
output: 
  ioslides_presentation:
    incremental: true
    transition: slower
    widescreen: true
    css: slides.css
    logo: ../gfx/ntu.png
bibliography      : ["../references.bib"]
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, eval = TRUE, message = FALSE, comment=NA)
options("kableExtra.html.bsTable" = T, digits = 3)
options(pillar.print_min = 5, pillar.print_max = 6)
library(tidyverse)
library(knitr)
library(kableExtra)
library(patchwork)
theme_set(theme_bw(base_size = 18) +
            theme(legend.position = "top", 
                  legend.justification = "right",
                  panel.grid = element_blank()))
```


## Regression models

<div style="float: left; width: 50%;">
- Often introduced as fitting lines to points.
- Limited perspective that makes more complex regression models, like generalised linear models, hard to understand.
- Backbone of statistical modelling: machine learning, multiple / simple linear regressions, t-tests, ANOVAs, ANCOVAs, MANCOVAs, time series models, path analysis, structural equation models, factor analysis
- Extension to generalised and multilevel linear models

</div>

<div style="float: right; width: 45%;">

```{r echo = F, out.width="100%"}
n <- 20
beta_0 <- 50
beta_1 <- 6
sigma <- 5
sim <- tibble(id = 1:n) %>%
    mutate(x = sample(0:5, size = n, replace = T),
           e = rnorm(n, mean = 0, sd = sigma)) %>% 
    mutate(y = beta_0 + beta_1 * x + e) %>%
  select(id, x, y)

ggplot(data =sim, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", se = F, colour = "red") +
  theme(axis.title = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank())
```

</div>

## Regression models

- A model of how the probability distribution of one variable, known as the *outcome variable*, varies as a function of other variables, known as the *explanatory* or *predictor variables*.
- The most basic type of regression models is the normal linear model.
- In normal linear models, we assume that the outcome variable is normally distributed and that its mean varies linearly with changes in a set of predictor variables.
- By understanding the normal linear model thoroughly, we can see how it can be extended to deal with data and problems beyond those that it is designed for.



## Normal linear model

<div style="float: left; width: 30%;">

$$
\begin{aligned}
y_i &\sim \mathcal{N}(\mu, \sigma^2)
\end{aligned}
$$
</div>

<div style="float: right; width: 65%;">
- $y$: observed univariate *outcome* variable
- Each observation $y_i$ is a sample from such a normal distribution (index by $i$)
- "$\sim$": "is proportional to" / "is a function of"
- $\mathcal{N}$: univariate normal distributed probability model with mean $\mu$ and variance $\sigma^2$ (or standard deviation $\sigma$)
- **Population parameters** are represented as Greek letters; i.e. what we want to know about our population given a set of assumptions about the nature of our data.
</div>



## Normal linear model

<div style="float: left; width: 30%;">

$$
\begin{aligned}
y_i &\sim \mathcal{N}(\mu_i, \sigma^2)\\
\mu_i &= \beta_0 + \beta_1 \cdot x_{1i} \\
\end{aligned}
$$
</div>

<div style="float: right; width: 65%;">
Value of $\mu_i$ is deterministic ("$=$") linear function of values of predictor variables $x$ for each observation $i$ and the unknowns $\beta_0$ (intercept) and $\beta_1$ (slope):

- $\beta_1$ directly depends on $x_{1i}$
- $\beta_0$ is "constant"
- If $x_{1i}$ takes on 0 $\mu_i = \beta_0$
- If $x_{1i}$ takes on 1 $\mu_i = \beta_0 + \beta_1$
- Hence, $\beta_1$ is the difference between $x_{1i}=1$ and $x_{1i}=0$: value we need to add to $\beta_0$ when $x_{1i}=1$ instead of $x_{1i}=0$.
</div>



## Normal linear model (general form)

$$
\begin{aligned}
y_i &\sim \mathcal{N}(\mu_i, \sigma^2)\\
\mu_i &= \beta_0 + \beta_1 \cdot x_{1i} + \beta_2 \cdot x_{2i} \dots \beta_k \cdot x_{ki} \dots \beta_K \cdot x_{Ki}
\end{aligned}
$$


## Normal linear model (general form)


$$
\begin{aligned}
y_i &\sim \mathcal{N}(\mu_i, \sigma^2)\\
\mu_i &= \beta_0 + \beta_1 \cdot x_{1i} + \beta_2 \cdot x_{2i} \dots \beta_k \cdot x_{ki} \dots \beta_K \cdot x_{Ki}\\
      & = \beta_0 + \sum_{k=1}^K \beta_k \cdot x_{ki}\\
\end{aligned}
$$


- Introducing $K$ in $x_{Ki}$ allows us to abbreviate the model equation.
- Probabilistic model (i.e. "$\sim$") of $y_1 \dots y_n$ conditional on $\overrightarrow{x_1}\dots\overrightarrow{x_n}$, $\overrightarrow{\beta} = \beta_0, \beta_1\dots\beta_K$, and $\sigma$
- $K$ values used to explain $y_i$. $\overrightarrow{x_i}$ are a set of $K$ *explanatory* variables. 
- Which ones do we know and which ones must be inferred?  


<!-- ## Normal linear model (general form) -->

<!-- <div style="float: left; width: 35%;"> -->

<!-- $$ -->
<!-- \begin{align} -->
<!-- y_i &\sim \mathcal{N}(\mu_i, \sigma^2)\\ -->
<!-- \mu_i &= \beta_0 + \sum_{k=1}^K \beta_k \cdot x_{ki}\\ -->
<!-- &\text{for } i \in 1\dots n -->
<!-- \end{align} -->
<!-- $$ -->
<!-- </div> -->

<!-- <div style="float: right; width: 60%;"> -->

<!-- - For every hypothetically possible value of the $K$ predictor variables, i.e. $\overrightarrow{x_{i'}}$, -->
<!-- there is a corresponding mean $\mu_{i'}$, i.e. $\mu_{i'} = \beta_0 + \sum_{k=1}^K \beta_k \cdot x_{ki'}$ -->

<!-- </div> -->



## Normal linear model (example)

> A model of how the mean of the distribution of reaction time values (rt) varies as a function of age and sex.

$$
\begin{align}
\text{rt}_i &\sim \mathcal{N}(\mu_i, \sigma^2)\\
\mu_i &= \beta_0 + \beta_\text{age} \cdot \text{age}_i + \beta_\text{sex} \cdot \text{sex}_i
\end{align}
$$

> For each observation $i \in 1\dots n$ where $n$ is total number of observations.

- Writing down (on paper) such a formula for an hypothetical normal linear model of blood pressure as a function of nutrition, age, and levels of physical activity.
- We will then write down the model description in RMarkdown.


## Simulation of normal distributed data

>- We can use `lm` to easily implement such a model.

```{r eval = F}
model <- lm(rt ~ age + sex, data = data)
```

- How do we know that `lm` returns accurate estimates for our model parameters?
- We can simulate data that exactly meet model assumptions (normal distribution, equal variance, independence etc.) with known model parameters.


## Simulation of normal distributed data

<div style="float: left; width: 50%;">

>- `rnorm` generates unimodal, symmetrically distributed values.
>- `r` in `rnorm` = random; `norm` = normal

```{r}
# Generate data
y <- rnorm(n = 10, mean = 550, sd = 10)
```

</div>

<div style="float: right; width: 45%;">

```{r echo = F, out.width="100%"}
ggplot(data = NULL, aes(x = y)) +
  geom_histogram() +
  coord_cartesian(xlim = c(500, 600),
                  ylim = c(0, 105)) 
```
</div>


## Simulation of normal distributed data

<div style="float: left; width: 50%;">

>- `rnorm` generates unimodal, symmetrically distributed values.
>- `r` in `rnorm` = random; `norm` = normal

```{r}
# Generate data
y <- rnorm(n = 100, mean = 550, sd = 10)
```

</div>

<div style="float: right; width: 45%;">

```{r echo = F, out.width="100%"}
ggplot(data = NULL, aes(x = y)) +
  geom_histogram() +
  coord_cartesian(xlim = c(500, 600),
                  ylim = c(0, 105))
```
</div>


## Simulation of normal distributed data

<div style="float: left; width: 50%;">

>- `rnorm` generates unimodal, symmetrically distributed values.
>- `r` in `rnorm` = random; `norm` = normal

```{r}
# Generate data
y <- rnorm(n = 1000, mean = 550, sd = 10)
```

</div>

<div style="float: right; width: 45%;">

```{r echo = F, out.width="100%"}
ggplot(data = NULL, aes(x = y)) +
  geom_histogram() +
  coord_cartesian(xlim = c(500, 600),
                  ylim = c(0, 105))
```


## Simulation of normal distributed data

<div style="float: left; width: 50%;">

>- `rnorm` generates unimodal, symmetrically distributed values.
>- `r` in `rnorm` = random; `norm` = normal

```{r}
# Decompose the mean
beta_0 = 500
beta_1 = 50
# Generate data
y <- rnorm(n = 1000, mean = beta_0 + beta_1, sd = 10)
```

</div>

<div style="float: right; width: 45%;">

```{r echo = F, out.width="100%"}
ggplot(data = NULL, aes(x = y)) +
  geom_histogram() +
  coord_cartesian(xlim = c(500, 600),
                  ylim = c(0, 110))
```



## Simulation of normal distributed data


```{r}
# Set parameter values
n <- 1000
beta_0 <- 500 
beta_1 <- 50
sigma <- 100
```

```{r}
# Random data for group 1
group_1 <- rnorm(n = n/2, mean = beta_0, sd = sigma)
# Random data for group 2
group_2 <- rnorm(n = n/2, mean = beta_0 + beta_1, sd = sigma)
```

```{r}
# Generate data
sim_data <- tibble(group_1 = group_1,
                   group_2 = group_2) 

# Preview data
glimpse(sim_data)
```


```{r}
# Change data format
sim_data <- pivot_longer(sim_data, cols = c(group_1, group_2), names_to = "x", values_to = "y")

# Preview data
glimpse(sim_data)
```


<!-- <div style="float: right; width: 45%;"> -->

<!-- ```{r echo = F, out.width="100%"} -->
<!-- ggplot(data = sim_data, aes(x = y, colour = x, fill = x)) + -->
<!--   geom_density(alpha = .25)  -->
<!-- ``` -->

<!-- </div> -->

## Simulation of normal distributed data

```{r}
# As a reminder, these are the parameter values
beta_0 <- 500 
beta_1 <- 50
sigma <- 100
```

```{r}
# Normal linear model of simulated data
model <- lm(y ~ x, data = sim_data)

coef(model) # Model coefficients
```

```{r}
sigma(model) # Standard deviation
```


<!-- <div style="float: right; width: 55%;"> -->

<!-- ```{r echo = F, out.width="100%", fig.cap="Estimated cell means with 95% CIs."} -->
<!-- plot_sim <- emmeans::emmeans(model, specs = "x") %>%  -->
<!--   as_tibble() %>%  -->
<!--   ggplot(aes(x = x,  -->
<!--              y = emmean, -->
<!--              ymin = lower.CL, -->
<!--              ymax = upper.CL)) + -->
<!--   geom_pointrange() + -->
<!--   coord_cartesian(ylim = c(575, 650)) + -->
<!--   scale_y_continuous(breaks = seq(500, 700, 10)) + -->
<!--   labs(y = "y") -->
<!-- plot_sim -->
<!-- ``` -->

<!-- </div> -->

## Simulation: exercise

> Work through the scripts 

>- `exercises/normal_model_simulation_1.R`
>- `exercises/normal_model_simulation_2.R`

> You will need to complete the code in those scripts.
> Then observe how changing the number of observations affects the model estimates.

## Real data 

- In real life we don't know the true parameter values.
- We also don't know the true process that generates the data.
- For the simulated data we know that the process: 
    - a normal distribution because `rnorm` samples normally distributed data
    - both groups have the same (equal) variance because the value for `sigma` was the same.
- We can use linear models to estimate parameter values from the data (i.e. we make assumptions about the process that generates the data).
- Maximum likelihood estimation: for which set of parameter values are the data most likely.

## Real data {.smaller}

<div style="float: left; width: 50%;">

>- Data may appear non-normal distributed because of small sample.
>- That's fine if we know that a normal distribution underlies the process that generates the data.

```{r}
blomkvist <- read_csv("../data/blomkvist.csv") %>% 
  select(id, sex, rt = rt_hand_d) %>% 
  mutate(logrt = log(rt))
```

```{r echo = F}
glimpse(blomkvist, width = 50)
```
>- Above data are by-participant means, meaning the central limit theorem applies.
>- Yet, reaction times are notoriously skewed [they are zero bound; see @baa08book].


</div>

<div style="float: right; width: 45%;">


```{r echo = F}
plot_blomkvist <- ggplot(data = blomkvist, aes(x = rt)) +
  geom_histogram(position = "dodge") +
  scale_x_continuous(breaks = seq(100, 2000, 200)) 

```

```{r echo = F, out.width="100%", fig.height=7}
plot_blomkvist / plot_spacer()
```

</div>



## Real data {.smaller}

<div style="float: left; width: 50%;">

>- Data may appear non-normal distributed because of small sample.
>- That's fine if we know that a normal distribution underlies the process that generates the data.

```{r}
blomkvist <- read_csv("../data/blomkvist.csv") %>% 
  select(id, sex, rt = rt_hand_d) %>% 
  mutate(logrt = log(rt))
```

```{r echo = F}
glimpse(blomkvist, width = 50)
```

>- Above data are by-participant means, meaning the central limit theorem applies.
>- Yet, reaction times are notoriously skewed [they are zero bound; see @baa08book].

</div>

<div style="float: right; width: 45%;">



```{r echo = F}
log_blomkvist <- ggplot(data = blomkvist, aes(x = logrt)) +
  geom_histogram(position = "dodge") +
  scale_x_continuous(breaks = seq(5, 8, .25)) 

```

```{r echo = F, out.width="100%", fig.height=7}
plot_blomkvist / log_blomkvist 
```

</div>


## Real data {.smaller}

<div style="float: left; width: 50%;">
>- Data are roughly normal distributed (there is some positive skew).
>- Variance is roughly the same in both groups.
>- We assume that the logarithm of the $rt$ denoted as $\text{logrt}_1, \text{logrt}_2\dots \text{logrt}_n$, $\text{ for } i \in 1\dots n$ is sampled from a log-normal distribution with a fixed and unknown mean $\mu$ ans standard deviation $\sigma$.

$$
\text{logrt}_i \sim \mathcal{N}(\mu_i, \sigma^2)
$$



</div>


<div style="float: right; width: 45%;">

```{r echo = F, out.width="100%", fig.height=7}
plot_blomkvist / log_blomkvist 
```

</div>




## Real data {.smaller}

<div style="float: left; width: 50%;">

> Predictor variable sex is denoted as $\text{sex}_1, \text{sex}_2, \dots \text{sex}_n$, $\text{ for } i \in 1\dots n$

$$
\mu_i = \beta_0 + \beta_\text{sex} \cdot \text{sex}_i
$$

> where $\text{sex}_i$ takes on $0$ for females and $1$ for males.


$$
\text{sex}_i = \left\{ 
  \begin{array}{ll}
  0, \text{ if sex}_i = \texttt{female}\\
  1, \text{ if sex}_i = \texttt{male}\\
  \end{array}
\right.
$$



</div>

<div style="float: right; width: 45%;">

```{r echo = F, out.width="100%", fig.height=7}
log_blomkvist / plot_spacer()
```

</div>


## Real data {.smaller}

<div style="float: left; width: 50%;">

> Predictor variable sex is denoted as $\text{sex}_1, \text{sex}_2, \dots \text{sex}_n$, $\text{ for } i \in 1\dots n$

$$
\mu_i = \beta_0 + \beta_\text{sex} \cdot \text{sex}_i
$$

> where $\text{sex}_i$ takes on $0$ for females and $1$ for males.


$$
\text{sex}_i = \left\{ 
  \begin{array}{ll}
  0, \text{ if sex}_i = \texttt{female}\\
  1, \text{ if sex}_i = \texttt{male}\\
  \end{array}
\right.
$$


- Because $\mu_i = \beta_0 + \beta_\text{sex} \cdot 0 = \beta_0$, $\beta_0$ is the average `logrt` for females.
- Because $\mu_i = \beta_0 + \beta_\text{sex} \cdot 1 = \beta_0 + \beta_\text{sex}$ is the average `logrt` for females, $\beta_\text{sex}$ is the difference in the averages `logrt` between males and females.



</div>

<div style="float: right; width: 45%;">

```{r echo = F, out.width="100%", fig.height=7}
log_blomkvist +
  facet_wrap(~sex, nrow = 2, labeller = label_both) +
  theme(strip.background = element_blank())
```

</div>

## Real data: exercise


- Complete exercise scripts
  - `exercises/normal_model_blomkvist_1.R`
  - `exercises/normal_model_blomkvist_2.R`
- Make sure you understand how the model coefficients relate to the visualisation.



## Compare simulated and real data


<div style="float: left; width: 45%;">

```{r eval = T}
# Set parameter values
beta_0 <- 500 
beta_1 <- 50
sigma <- 100
```

```{r echo = F}
# Generate data
data <- tibble(group_1 = rnorm(n = n/2, 
                               mean = beta_0, 
                               sd = sigma),
               group_2 = rnorm(n = n/2, 
                               mean = beta_0 + beta_1, 
                               sd = sigma)) 

# Create long data format
data <- data %>% 
  pivot_longer(starts_with("group"), 
               names_to = "x", 
               values_to = "y")

```


```{r}
# Inspect data
glimpse(data, width = 50)
```


```{r eval = T}
# Specify model
model <- lm(y ~ x, data = data)
```

```{r}
# Model coefficients
coef(model)
```


</div>

<div style="float: right; width: 45%;">

```{r}
# Load and transform data
blomkvist <- read_csv("../data/blomkvist.csv") %>% 
  mutate(log_rt = log(rt_hand_d)) %>% 
  select(sex, log_rt)
```

```{r}
# Inspect data
glimpse(blomkvist, width = 50)
```


```{r}
# Specify model
model <- lm(log_rt ~ sex, data = blomkvist)
```

```{r}
# Model coefficients
coef(model)
```


</div>

## Interpreting coeffcients on the log scale {.smaller}


<div style="float: left; width: 45%;">

> Slope coef is no longer additive (linear scale) but multiplicative (exponential scale)

```{r}
# Specify model
model <- lm(log_rt ~ sex, data = blomkvist)

# Model coefficients
(coefs <- as.vector(coef(model)))
```

> I.e. `logrt` is $\beta_1$ times longer.


</div>

<div style="float: right; width: 50%;">


```{r}
# Converting coefficients to msecs
intercept <- coefs[1]
slope <- coefs[2]
```

> Exponentional function `exp` un-logs coefficients.

```{r}
exp(intercept) # female group in msecs
```

```{r}
exp(intercept + slope) # male group in msecs
```


```{r}
exp(intercept + slope) - exp(intercept) # in msecs
```



</div>




## Interpreting coeffcients on the log scale {.smaller}


<div style="float: left; width: 47%;">

> For any given value $\text{sex}'$, the distribution of `logrts` is normally distributed with a mean $\mu' = \beta_0 + \beta_1 \cdot x'$ and standard deviation $\sigma$.

```{r}
coef(model)
```

```{r}
exp(intercept) # female group in msecs
```


</div>

<div style="float: right; width: 47%;">

>- If sex changes by $\Delta_x$, the mean of the normal distribution over `logrts` changes by
exactly $\beta_1 \cdot \Delta_x$.
>- If sex changes by $\Delta_x=1$, the mean of the normal distribution over `logrts` changes by exactly $\beta_1$.


```{r}
exp(intercept + slope) - exp(intercept) 
```

> Complete 

- `exercises/interpreting_coefficients_1.R`
- `exercises/interpreting_coefficients_2.R`


</div>


## Maximum likelihood estimation 

> If we assume the model


$$
y_i \sim \mathcal{N}(\mu_i, \sigma^2) \text{, for } i \in 1\dots n\\ 
\mu_i = \beta_0 + \sum_{k=1}^K \beta_k \cdot x_{ki}
$$

> how can we estimate values of $K+2$ unknowns. 

> The maximum likelihood estimators of the $K + 2$ unknowns, which we can denote by

$$
\hat\beta_0,\hat\beta_1\dots\hat\beta_K,\hat\sigma,
$$

> are the set of values that make the observed data most probable.


## Maximum likelihood estimation

> The maximum likelihood values of coefficients $\hat\beta_0,\hat\beta_1\dots\hat\beta_K$ are those values of $\beta_0,\beta_1\dots\beta_K$ variables that minimize *residual sum of squares*

$$
\text{RSS} = \sum_{i=1}^n \mid y_i - \mu_i\mid^2,
$$

> where $\mu_i$ was defined before.

> This can be easily simulated using grid approximation for models with small sets of unknowns: see script `exercises/mle.R` 



## Fitting a normal linear model using `lm` 

>- For larger sets of parameters we can use maximum likelihood estimation as implemented in `lm`.
>- We'll continue with the @blomkvist2017reference `rt` data for simplicity.

```{r echo = F}
blomkvist <- read_csv("../data/blomkvist.csv") %>% 
  select(id, sex, age, rt = rt_hand_d) %>% 
  drop_na()
```


```{r}
model <- lm(rt ~ sex + age, data = blomkvist)
```

>- Maximum likelihood estimators of $\beta_0$, $\beta_\text{sex}$ and $\beta_\text{age}$ are as follow:

```{r}
(coefs <- coef(model))
```




## Fitting a normal linear model using `lm` 


> Once we have $\hat\mu$, the maximum likelihood estimate of $\sigma$, denoted as $\hat\sigma$, is:

$$
\hat\sigma = \sqrt{\frac{1}{n-K-1} \cdot \sum_{i=1}^n\mid y_i-\hat\mu_i\mid^2}
$$

>- $K$ is the number of predictors
>- $n$ is the number of observations
>- $\sum_{i=1}^n\mid y_i-\hat\mu_i\mid^2$ is the sum of the squared differences of the observed $y_i$ and predicted data $\hat\mu_i$
>- Complete script `exercises/calculating_sigma.R`




## Hypothesis testing and confidence intervals

> If $\beta_k$ is the **hypothesized** true value of the coefficient, and $\hat\beta_k$ is the estimated coefficient, then

$$
\frac{\hat\beta_k - \beta_k}{\hat{\text{se}}_k} \sim t_{n-K-1}, 
$$

> which tells us that if the true value of the coefficient was some hypothesized value $\beta_k$, then we expect the *t*-statistic to have a certain range of values with high probability.

## Hypothesis testing and confidence intervals

> We will test a hypothesis about the coefficient for sex in the model above.

<div style="float: left; width: 47%;">

```{r}
# Extract the coefficients table from summary
coefs <- summary(model)$coefficients
# MLE for sex
(beta_sex <- coefs['sexmale', 'Estimate'])
```

```{r}
# Standard error
(se_sex <- coefs['sexmale', 'Std. Error'])
```

</div>

<div style="float: right; width: 47%;">

> If we hypothesize the true value of the coefficient is exactly 1.0, then our *t*-statistic is

```{r}
(t <- (beta_sex - 1.0)/se_sex)
```

</div>


$$
\frac{\hat\beta_k - \beta_k}{\hat{\text{se}}_k} = \frac{-58.9 - 1.0}{18.5} = -3.24
$$

## *p*-values

>- The *p*-value for the hypothesis is the probability of getting a value *as or more extreme* than the absolute value of *t*-statistic in the *t*-distribution.
>- This is the area under the curve in the *t*-distribution that is as or more extreme than the absolute value of *t*-statistic.
>- It tells us how far into the tails of the distribution the *t*-statistic is.

```{r}
n <- nrow(blomkvist)
K <- 2
pt(abs(t), df = n - K - 1, lower.tail = FALSE) * 2
```

## Hypothesis testing and confidence intervals 


> **Null hypothesis significance testing**: we hypothesize the true value of the coefficient is exactly 0.0, then our *t*-statistic is

$$
\frac{\hat\beta_k - \beta_k}{\hat{\text{se}}_k} = \frac{-58.9 - 0.0}{18.5} = \frac{-58.9}{18.5} = -3.19
$$

```{r}
(t <- (beta_sex - 0.0)/se_sex)
```

```{r}
n <- nrow(blomkvist)
K <- 2
pt(abs(t), df = n - K - 1, lower.tail = FALSE) * 2
```


## *p*-values

>- Work through script `exercise/t_values.R` 
>- Calculate *t* and *p*-value for $\hat\beta_\text{age}$ testing the null hypothesis that the true value $\beta_\text{age}$ is 0.0.
>- Verify your calculation using

```{r}
summary(model)$coefficients
```




## *t*-distribution


```{r echo = F,  out.width="80%", fig.height=4}
t <- abs(beta_sex / se_sex)
n <- nrow(blomkvist)
K <- 1
df <- n - K - 1
p_value <- pt(t, df = df, lower.tail = FALSE) * 2

# Hypothetical t values
t_grid <- seq(0, 10, .1) 
p_values <- pt(t_grid, df = df, lower.tail = FALSE) * 2

p <- ggplot(data = NULL, aes(x = t_grid, y = p_values)) +
  geom_line() +
  geom_hline(yintercept = 0.05, linetype = 'dotted') +
  scale_x_continuous(breaks = seq(0, 20, 1)) +
  labs(x = 'Theoretical t-value',
       y = 'p-value',
       subtitle = bquote("t distribution for"~"Df" == .(df)));p 

```


## *t*-distribution 

```{r echo = F,  out.width="80%", fig.height=4}
label <- deparse(bquote(paste("t" == .(round(t, 2)))))

p2 <- p + geom_segment(aes(x = t+1, y = .2, xend = t, yend = 0.02),
                  arrow = arrow(length = unit(0.35, "cm")),
               colour = "red") +
  geom_label(aes(x = t + 1, y = 0.3, label = label), parse = T); p2
```

## *t*-distribution 

```{r echo = F,  out.width="80%", fig.height=4}
label_2 <- deparse(bquote(paste("p" == .(round(p_value, 3)))))
p2 + geom_hline(yintercept = p_value, colour = "red", linetype = "dashed") +
  geom_label(aes(x = .65, y = 0.05, label = label_2), parse = T) 

```


## *t*-distribution

```{r echo = F,  out.width="80%", fig.height=4}
label_2 <- deparse(bquote(paste("p" == .(round(p_value, 3)))))
p2 + geom_hline(yintercept = p_value, colour = "red", linetype = "dashed") +
  geom_label(aes(x = .65, y = 0.05, label = label_2), parse = T) +
  annotate("label", x = 8, y = .9, label = "Work through script\n exercise/t_distribution.R")

```




## *t*-distribution


```{r echo = F, out.width="80%", fig.height=4}
t_grid <- seq(0, 15, .1) 
df <- c(1, 2, 5, 8, 10, 100, 1000, 10000)

data_df <- map_dfr(df, ~pt(t_grid, df = .x, lower.tail = F) %>% tibble(t_grid = t_grid, p = . * 2) %>%  
    mutate(df = .x)) 

ggplot(data = data_df, aes(x = t_grid, y = p, colour = factor(df))) +
  geom_line() +
  labs(x = "Theoretical t-value",
       y = "p-value",
       subtitle = "t-distribution",
       colour = bquote("Df")) +
  scale_colour_viridis_d() +
  scale_x_continuous(breaks = seq(0, 20, 1)) +
  geom_hline(yintercept = 0.05, linetype = "dotted") +
  theme(legend.position = c(.925,.55),
        legend.title = element_text(size = 10),
        legend.text = element_text(size = 10))


```


## Confidence intervals {.smaller}

<div style="float: left; width: 45%;">

>- The set of all hypotheses that we do not rule out at the $\alpha < 0.05$ level of significance is known as the 95% confidence interval.
>- The formula for the confidence interval on coefficient $k$ is

$$
\hat\beta_k \pm \tau_\nu \cdot \hat{\text{se}}_k
$$

>- $\tau_\nu$ is value in a *t*-distribution (with $\nu$ degrees of freedom) below which is 95% of the area under the curve: i.e. $\tau\approx1.96$ for large dfs.


</div>

<div style="float: right; width: 50%;">

```{r echo = F, out.width="100%", fig.height=5}
t_grid <- seq(-4, 4, .1) 
df <- c(1, 5, 10, 1000)

data_df <- map_dfr(df, ~dt(t_grid, df = .x) %>% tibble(t_grid = t_grid, p = .) %>%  
    mutate(df = .x)) %>% 
  mutate(across(df, as.factor))

ggplot(data = data_df, aes(x = t_grid, y = p, colour = df)) +
  geom_line() +
  labs(x = "t-value",
       y = "density",
       subtitle = "t-distribution") +
  scale_colour_viridis_d() +
  scale_x_continuous(breaks = seq(-20, 20, 1)) +
  geom_vline(xintercept = c(-1.96, 1.96), linetype = "dotted") +
  theme(legend.position = "right",
        legend.justification = "top")

```

>- $\tau$ times standard errors above and below ('$\pm$') $\hat\beta_k$ are lower and upper bound of the confidence interval.

</div>


## Confidence intervals 

<div style="float: left; width: 50%;">

```{r}
(coefs <- coef(summary(model)))
```

```{r}
beta_sex <- coefs[2,1]
se_sex <- coefs[2,2]
```

> We can calculate $\tau$ using `qt`

```{r}
(tau <- qt(.975, df = n - K - 1))
```

</div>

<div style="float: right; width: 45%;">

> We then obtain the confidence intervals as follows

```{r}
beta_sex + c(-1, 1) * se_sex * tau
```


```{r}
confint(model, parm = 'sexmale',  level = .95)
```

> Open and complete script `exercises/confidence_interval.R`

</div>


## Confidence intervals: a simulation {.smaller}

<div style="float: left; width: 50%;">

>- Work through script `exercises/confidence_interval_simulation.R`
>- Change the number of simulations to, one at a time, `n_sims = 20, 50, 100, 500, 1000`
>- Notice how often the CI does not contain the true parameter value $\mu$.
>- How many % of simulations contain $\mu$?


```{r echo = F}
set.seed(324)
# Set simulation parameters
mu <- 600 # True parameter value
n <- 1000 # Number of observations per experiment
n_sims <- 10 # Number of hypothetical experiments
# Empty data frame for results
sims <- tibble()
# Run n_sims simulations
for(i in 1:n_sims){
  # Simulate normal distributed data
  y <- rnorm(n, mean = mu, sd = 100)
  # Get MLEs
  m <- lm(y ~ 1)
  # Extract CI
  cis <- confint(m) %>% as.vector()
  # Store results
  sims <- tibble(sim_id = i, 
                 lower = cis[1], 
                 upper = cis[2]) %>% 
    bind_rows(sims)
}
```

</div>

<div style="float: right; width: 45%;">


```{r echo = FALSE, out.width="100%", fig.height=7}
mutate(sims, 
       mu_in_ci = ifelse(mu < lower | mu > upper, "no", "yes")) %>% 
  ggplot(aes(x = sim_id, 
             ymin = lower, 
             ymax = upper, 
             colour = mu_in_ci)) +
  geom_errorbar(width = 0) +
  geom_hline(yintercept = mu, linetype = "dotted", colour = "blue") +
  scale_colour_manual(values = c("no" = "red", "yes" =" black")) +
  scale_x_continuous(breaks = seq(1, n_sims, 1)) +
  scale_y_continuous(limits = c(580, 620)) +
  labs(colour = bquote("95% CI includes"~mu*":"),
        caption = bquote(.(n_sims)~"simulations"),
       x = "Experiment id (simulation)",
       y = "Parameter estimate") +
  coord_flip()

```
</div>



## Confidence intervals: a simulation {.smaller}

<div style="float: left; width: 45%;">

```{r echo = F}
mu <- 600 # True parameter value
n_sims <- 1000 # Number of hypothetical experiments
n <- 1000 # Number of observations per experiment
sims <- tibble()
for(i in 1:n_sims){
  y <- rnorm(n, mean = mu, sd = 100)
  m <- lm(y ~ 1)
  cis <- confint(m) %>% as.vector()
  sims <- tibble(sim_id = i, lower = cis[1], upper = cis[2]) %>% 
    bind_rows(sims)
}
```

```{r echo = FALSE, out.width="100%", fig.height=7}
sims <- mutate(sims, mu_in_ci = ifelse(mu < lower | mu > upper, "no", "yes"))
ggplot(sims, aes(x = sim_id, 
                 ymin = lower, 
                 ymax = upper, 
             colour = mu_in_ci)) +
  geom_errorbar(width = 0) +
  geom_hline(yintercept = mu, linetype = "dotted", colour = "blue") +
  scale_colour_manual(values = c("no" = "darkred", "yes" =" grey50")) +
  scale_y_continuous(limits = c(580, 620)) +
  labs(colour = bquote("95% CI includes"~mu*":"),
       caption = bquote(.(n_sims)~"simulations"),
       x = "Experiment id (simulation)",
       y = "Parameter estimate") +
  coord_flip()
```

</div>

<div style="float: right; width: 45%;">

```{r}
count(sims, mu_in_ci) %>% 
  mutate(prop = n / n_sims)
```

- When number of experiments approaches $\infty$ and we calculate the 95% CI for each experiment, 95% of these CIs contain $\mu$.
- Note though, 5% of the time, the confidence interval does not contain $\mu$!
- What does this mean for real (as opposed to simulated) experiments?


</div>



## Predictions 

> Using $\hat\beta_0,\hat\beta_1\dots\hat\beta_K$ and $\hat\sigma^2$, then for any new vector of predictor variables $\overrightarrow{x}_{i'}$, the corresponding $y_i$ is 

$$
y_{i'} \sim \mathcal{N}(\hat\mu_{i'}, \sigma^2)\\ 
\hat\mu_{i'} = \hat\beta_0 + \sum_{k=1}^K \hat\beta_k \cdot x_{i'k}
$$


## Predictions 


> For our model coefficients

```{r}
coef(model)
```


> For $K = 2$ we can rewrite / simplify / predict rts for a hypothetical female age 95.

$$
\begin{aligned}
\hat\mu_i &= \hat\beta_0 + \hat\beta_\text{sex} \cdot \text{sex}_{i} + \hat\beta_\text{age} \cdot \text{age}_{i}\\
&=338.64 + -58.93 \cdot \text{sex}_{i} + 5.75 \cdot \text{age}_{i}\\
&=338.64 + -58.93 \cdot 0 + 5.75 \cdot 95\\
&=338.64 + 5.75 \cdot 95\\
&=885
\end{aligned}
$$

> To calculate $\hat\mu_{i'}$ for any vector of predictor variables $\overrightarrow{x}_{i'}$ 

```{r}
newdata <- tibble(sex = "female", age = 95)
predict(model, newdata = newdata) 
```



## Prediction confidence intervals 

>- We can calculate confidence intervals for the predicted values of $\mu_{i'}$.
>- The confidence interval for $\mu_{i'}$ is as follows:

$$
\hat\mu_{i'} \pm \tau_\nu \cdot \hat{\text{se}}_{\mu_{i'}}
$$

> where $\hat{\text{se}}_{\mu_{i'}}$ is the standard error term that corresponds to $\hat\mu_{i'}$.

>- We can obtain these by using `interval = 'confidence'` in the `predict` function.

```{r}
predict(model, interval = 'confidence', newdata = newdata)
```


## Prediction confidence intervals: exercise

> Work through scripts

>- `exercises/predictions.R`
>- `exercises/predictions_with_cis.R`


## Varying intercepts model 

> When we include `sex` as an explanatory variable in additional to a continuous predictor variable (e.g. `age`), the model is as follows. 

$$
\text{rt}_i \sim \mathcal{N}(\mu_i, \sigma^2)\\
\mu_i = \beta_0 + \beta_\text{sex} \cdot \text{sex}_{i} + \beta_\text{age} \cdot \text{age}_{i}
$$

> With sexes denoted by $\text{sex}_{1},x_{2}\dots \text{sex}_{i}\dots \text{sex}_{n}$ and age by $\text{age}_{1},\text{age}_{2}\dots \text{age}_{2i}\dots \text{age}_{2n}$ $\text{ for } i \in 1\dots n$.


## Varying intercepts model 

> To implement this model using `lm` we would do the following.

```{r}
model_2 <- lm(rt ~ sex + age, data = blomkvist)
coef(model_2)
```
- Note that `sex` is dichotomous categorical and `age` is continuous. 
- Normal distribution over `rt` for any given combination of sex and age.
- If `sex` changes by one unit, when age is held constant, then the average value of the corresponding distribution of `rt` changes by $\beta_\text{sex}$.
- Conversely, if `age` changes by one unit, when `sex` is held constant, the average value of the distribution of `rt` changes by $\beta_\text{age}$.




## Varying intercepts model

> Given that $\text{sex}_{i}$ takes that value of 0 when the gender is female and 1 when gender is male, this model can be written as follows.


$$
\text{rt}_i \sim \mathcal{N}(\mu_i, \sigma^2) \\
\mu_i =  \left\{ 
\begin{array}{ll}
\beta_0 + \beta_\text{age} \cdot \text{age}_{i}, \text{ if sex}_i = 0 \texttt{ [female]}\\
\beta_0 + \beta_\text{sex} \cdot \text{age}_{i} + \beta_\text{age} \cdot \text{age}_{i}, \text{ if sex}_i = 1\texttt{ [male]}
\end{array}
\right.
$$

> This is a *varying intercepts* model.



## Varying intercepts model

```{r echo = F}
model <- lm(rt ~ age, data = blomkvist)

blomkvist$pred <- predict(model)
blomkvist$resid <- residuals(model)

plot <- ggplot(blomkvist, aes(x = age, y = pred) ) +
  stat_smooth(method = "lm", linewidth = 1, colour = "darkred") +
  labs(y = "rt", 
       subtitle = bquote("rt"[i]~"~"~"N("*mu["i"]*","~sigma^2*"),"~mu["i"] == beta[0]~+~beta["age"]~"*"~"age"["i"])) +
  geom_point(aes(x = age, y = rt), size = .5);plot

```


## Varying intercepts model

```{r echo = F}
model <- lm(rt ~ age + sex, data = blomkvist)

blomkvist$pred <- predict(model)
blomkvist$resid <- residuals(model)

plot <- ggplot(blomkvist, aes(x = age, y = pred, colour = sex) ) +
  stat_smooth(method = "lm", linewidth = 1) +
  labs(y = "rt", 
       subtitle = bquote("rt"[i]~"~"~"N("*mu["i"]*","~sigma^2*"),"~mu["i"] == beta[0]~+~beta["age"]~"*"~"age"["i"]~+~beta["sex"]~"*"~"sex"["i"])) +
  geom_point(aes(x = age, y = rt, colour = sex), size = .5) +
  scale_colour_viridis_d(end = .6) +
  theme(legend.position = "right",
        legend.justification = "bottom");plot

```


## Varying intercepts and varying slopes model

The varying-intercepts model is written as follows. For $i \in 1\dots n$

$$
\begin{aligned}
\text{rt}_i &\sim N(\mu_i, \sigma^2)\\
\mu_i &= \beta_0 + \beta_\text{sex} \cdot \text{sex}_{i} + \beta_\text{age} \cdot \text{sex}_{i}\\
\end{aligned}
$$

The following is a varying intercepts and varying slopes model:

$$
\begin{aligned}
\text{rt}_i &\sim N(\mu_i, \sigma^2)\\ 
\mu_i &= \beta_0 + \beta_\text{sex} \cdot \text{sex}_{i} + \beta_\text{age} \cdot \text{age}_{i} + \underbrace{\beta_\text{sex,age} \cdot \text{sex}_{i} \cdot \text{age}_{i}}_\text{interaction}
\end{aligned}
$$

We effectively have a third predictor that is the product of $\text{sex}_{i} \cdot \text{age}_{i}$; i.e. the interaction.


## Varying intercepts and varying slopes model

Using `sex` and `age` as predictors, we can do the following to perform a varying-intercepts and varying-slopes model:

```{r}
model_3 <- lm(rt ~ sex + age + sex:age, data = blomkvist)
```

Or slightly shorter:

```{r}
model_3 <- lm(rt ~ sex * age, data = blomkvist)
```

## Varying intercepts and varying slopes model


<div style="float: left; width: 45%;">

```{r echo = F, out.width="100%", fig.height=7}
model <- lm(rt ~ age + sex, data = blomkvist)

blomkvist$pred <- predict(model)
blomkvist$resid <- residuals(model)

plot <- ggplot(blomkvist, aes(x = age, y = pred, colour = sex) ) +
  stat_smooth(method = "lm", linewidth = 2) +
  labs(y = "rt", 
       subtitle = bquote("rt"[i]~"~"~"N("*mu["i"]*","~sigma^2*"),"~mu["i"] == beta[0]~+~beta["age"]~"*"~"age"["i"]~+~beta["sex"]~"*"~"sex"["i"])) +
  geom_point(aes(x = age, y = rt, colour = sex), size = 1) +
  scale_colour_viridis_d(end = .6) +
  theme(plot.subtitle = element_text(size = 16));plot

```

</div>

<div style="float: right; width: 45%;">

```{r echo = F, out.width="100%", fig.height=7}
model <- lm(rt ~ age * sex, data = blomkvist)

blomkvist$pred <- predict(model)
blomkvist$resid <- residuals(model)

plot <- ggplot(blomkvist, aes(x = age, y = pred, colour = sex) ) +
  stat_smooth(method = "lm", linewidth = 2) +
  labs(y = "rt", 
       subtitle = bquote("rt"[i]~"~"~"N("*mu["i"]*","~sigma^2*"),"~mu["i"] == beta[0]~+~beta["age"]~"*"~"age"['i']~+~beta["sex"]~"*"~"sex"['i']~+~beta["age,sex"]~"*"~"sex"['i']~"*"~"age"['i'])) +
  geom_point(aes(x = age, y = rt, colour = sex), size = 1) +
  scale_colour_viridis_d(end = .6) +
  theme(plot.subtitle = element_text(size = 16));plot

```

</div>


## Varying intercepts and varying slopes model


<div style="float: left; width: 50%;">

> From the varying-intercepts and varying-slopes model, the intercept terms for `female` and `male` are


```{r}
model_3 <- lm(rt ~ sex * age, data = blomkvist)
```


```{r}
(betas <- coef(model_3))
```


```{r echo = F}
betas <- as.vector(coef(model_3))
```


```{r}
# females
betas[1]
```


</div>

<div style="float: right; width: 42%;">




```{r}
# males
betas[1] + betas[2]
```


> The slope terms are as follows:

```{r}
# females
betas[3]
```


```{r}
# males
betas[3] + betas[4]
```

</div>

## Varying intercepts and varying slopes model


> From the varying-intercepts and varying-slopes model, the intercept terms for `female` and `male` are

```{r}
model_3 <- lm(rt ~ sex * age, data = blomkvist)
```


<div style="float: left; width: 45%;">

```{r}
(betas <- coef(model_3))
```

```{r echo = F}
betas <- as.vector(coef(model_3))
```


```{r}
# females
betas[1]
```


```{r}
# males
betas[1] + betas[2]
```

</div>

<div style="float: right; width: 45%;">

> The slope terms are as follows:

```{r}
# females
(betas[1] + betas[3]) - betas[1]
```


```{r}
# males
(betas[1] + betas[3] + betas[4]) -betas[1]
```


</div>


## Varying intercepts and varying slopes model

> Work through scripts
 
>- `exercises/varying_intercepts_model.R`
>- `exercises/varying_slopes_model.R`


## Polychotomous predictors {.smaller}

> Categorical predictor variables with more than two levels (i.e. dichotomous like `sex`).


```{r eval = T, echo = F}
# Load and pre-process data
blomkvist <- read_csv("../data/blomkvist.csv") %>% 
  select(id, smoker, rt = rt_hand_d) %>% 
  mutate(across(smoker, as.factor)) %>% 
  drop_na()
```


<div style="float: left; width: 100%;">

> Using `smoker` as single categorical predictor variable, the linear model would be as follows.

$$
\text{rt}_i \sim N(\mu_i, \sigma^2), \mu_i = \beta_0 + \beta_\text{smoker[1]} \cdot \text{smoker}_{1i} + \beta_\text{smoker[2]} \cdot \text{smoker}_{2i}, \text{ for } i \in 1\dots n.
$$

> where $\text{smoker}_{1i}$, $\text{smoker}_{2i}$ are as follows.

$$
\text{smoker}_{1i}, \text{smoker}_{2i}
 =  \left\{ 
\begin{array}{ll}
0,0 \text{ if smoker}_i = \texttt{former}\\
1,0 \text{ if smoker}_i = \texttt{no}\\
0,1 \text{ if smoker}_i = \texttt{yes}\\
\end{array}
\right.
$$
</div> 

> The number of coefs for polychotomous predictor is $K-1$ where $K$ is number of levels of the predictor.

```{r}
# Categories of smoker
levels(blomkvist$smoker) #  3 levels
```


## Polychotomous predictors {.smaller}

> Using `lm`, we would simply do as follows.

```{r}
model_smoker <- lm(rt ~ smoker, data = blomkvist)
(betas <- coef(model_smoker))
```

```{r echo = F}
betas <- as.vector(coef(model_smoker))
```

> Default contrasts for polychotomous predictor used by `lm`.

```{r}
contrasts(blomkvist$smoker)
```

> The intercept term is the predicted average when `smoker` is `former` (i.e. row with only 0s).



## Polychotomous predictors {.smaller}


> Using `lm`, we would simply do as follows.

```{r}
model_smoker <- lm(rt ~ smoker, data = blomkvist)
(betas <- coef(model_smoker))
```



```{r echo = F}
betas <- as.vector(coef(model_smoker))
```


> The predicted mean of the weight distribution for `no`

$$
\hat\beta_0 + \hat\beta_1 \cdot 1 + \hat\beta_2 \cdot 0 
$$




```{r}
betas[1] + betas[2] * 1 + betas[3] * 0
```



## Polychotomous predictors {.smaller}


> Using `lm`, we would simply do as follows.

```{r}
model_smoker <- lm(rt ~ smoker, data = blomkvist)
(betas <- coef(model_smoker))
```

```{r echo = F}
betas <- as.vector(coef(model_smoker))
```

> When `smoker` is `yes`, the predicted mean of the rt distribution for smokers is 

$$
\hat\beta_0 + \hat\beta_1 \cdot 0 + \hat\beta_2 \cdot 1 
$$


```{r }
betas[1] + betas[2] * 0 + betas[3] * 1
```


> Exercise: work through script `exercises/polychotomous_predictor.R`

</div>

<!-- Include anova to test overall effect (more in model comparisons) -->


## Contrast coding {.smaller}

> Change contrast coding so intercept represents the average across both groups and to get desired copmarisons [for discussion see @schad2020capitalize;@brehm2022contrast].


```{r}
coef(model_smoker)
```


```{r echo=F}
blomkvist <- mutate(blomkvist, across(smoker, factor))
```


```{r}
# Default: treatment contrast
contrasts(blomkvist$smoker)
```

```{r eval = F}
contr.treatment(3, base = 2) # change base level
```

```{r echo = F}
contrasts(blomkvist$smoker) <- contr.treatment(3, base = 2) # change base level
row.names(contrasts(blomkvist$smoker)) <- levels(blomkvist$smoker)
colnames(contrasts(blomkvist$smoker)) <- c("former", "yes")
contrasts(blomkvist$smoker)
```


```{r eval = F}
contr.helmert(3) / 2 # Helmert contrast
```

```{r echo = F}
contrasts(blomkvist$smoker) <- contr.helmert(3) / 2 # Helmert contrast
row.names(contrasts(blomkvist$smoker)) <- levels(blomkvist$smoker)
colnames(contrasts(blomkvist$smoker)) <- c("former vs no", "former / no vs yes")
contrasts(blomkvist$smoker)
```


```{r eval = F}
contr.sum(3) / 2 # sum contrast
```

```{r echo = F}
contrasts(blomkvist$smoker) <- contr.sum(3) / 2 # sum contrast
row.names(contrasts(blomkvist$smoker)) <- levels(blomkvist$smoker)
colnames(contrasts(blomkvist$smoker)) <- c("former vs yes", "no vs yes")
contrasts(blomkvist$smoker)
```


```{r}
model <- lm(rt ~ smoker, data = blomkvist) # Re-fit model with sum contrast
coef(model)
```

```{r}
coef(model_smoker) # model with treatment contrasts
```

> Exercise: work through script `exercises/contrast_coding.R`


## Recap

- Normal model assumes a single normal distributed data generating process.
- Log-normal transformation is often used to correct positive skew (assuming a log-normal process).
- MLE provides estimates for unknown population parameters.
- We can specify models with continues and / or categorical predictors.
- Combinatioins are varying intercepts and / or varying slopes.
- Read lecture notes on NOW [also @andrews2021doing].
- Model comparisons: has age a stronger effect for females than for males?



## Homework

>- We will start preparing your formative assessment next week.
>- Review the [details for the formative assessment.](https://now.ntu.ac.uk/d2l/le/content/887088/viewContent/9653015/View)

> *Using a data-set of your own choice, perform and report a linear regression analysis to address a set of theoretical questions of your choice that relate to your chosen data-set*

>- Find a suitable data set you can use for the formative assessment.
>- Create an outline for your report (what do you need to include?).

## References {.smaller}

<style>
slides > slide { overflow: scroll; }
slides > slide:not(.nobackground):after {
  content: '';
}
</style>


