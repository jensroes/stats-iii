---
title: 'Model comparison'
author: '<span style="font-size: 40px; font-face: bold">Jens Roeser</span>'
output: 
  ioslides_presentation:
    incremental: true
    transition: slower
    widescreen: true
    css: slides.css
    logo: ../gfx/ntu.png
bibliography      : ["../references.bib"]
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, eval = TRUE, message = FALSE, comment=NA)
options("kableExtra.html.bsTable" = T, digits = 3)
options(pillar.print_min = 5, pillar.print_max = 6)
library(tidyverse)
library(knitr)
library(kableExtra)
library(patchwork)
library(broom)
theme_set(theme_bw(base_size = 18) +
            theme(legend.position = "top", 
                  legend.justification = "right",
                  panel.grid = element_blank()))
```



```{r echo = F}
# Load and transform data
blomkvist <- read_csv("../data/blomkvist.csv") %>% 
  select(id, sex, age, rt = rt_hand_d) %>% 
  mutate(log_rt = log(rt)) %>% 
  drop_na()
```



## Overview

> By the end of these workshops and your own reading, you should be able to

>- Describe the meaning of and calculate likelihood
>- Using unexplained variance to assess model fit
>- Describe the limitations of $R^2$ as model fit statistic
>- Use likelihood ratio test (*F*-test) and information criteria to compare nested models


## Why do we need model comparion? 

<div style="float: left; width: 35%;">

> "All models are wrong, but some are useful"
>
> George E.P. Box

</div>

<div style="float: right; width: 55%;">

>- Models are statistical representations of a hypothesis.
>- We want to compare the usefulness of alternative models.
>- Can my model accurately predict future outcomes?
>- Parsimony: As complex as necessary, as simple as possible.
</div>

## Ockham's razor: the principle of parsimony 

<div style="float: left; width: 55%;">

>- Is it worth assuming a more complex model?
>- Philosopher William of Ockham (1285-1347/49): 

> "Plurality should not be posited without necessity."

</div>

<div style="float: right; width: 40%;">
```{r echo=F, out.width="100%"}
include_graphics("../gfx/ockham.jpg")
```

</div>

>- Precedence to simplicity: of two competing theories, the simpler explanation of an entity is to be preferred. 
>- We don't assume more complex theories, if there is not sufficient evidence.



```{r overview, echo = F, eval = F}
read_csv("overview.csv") %>% 
  kable(caption = "Popular model evaluation techniques") %>% 
  kable_styling(full_width = T, bootstrap_options = c("responsive", "hover", "striped"))
```



```{r eval = FALSE, echo=FALSE}
- Model evaluation: check how good the model fits the data
- Testing whether a predictor explains variance and make the model beter
- Hypothesis testing: testing whether one model is better than a simpler model with otherwise compareable properties.

```





## Probabilistic generative models 

>- In any statistical analysis, we assume our data are drawn from some probability distribution.
>- This is what we mean by *statistical model*, a *probabilistic generative model* like

$$
y_i \sim \mathcal{N}(\mu_i, \sigma^2), \mu_i = \beta_0 + \beta_1 \cdot x_i
$$

>- A model of the *statistical population* could also be described as the true generative model.
>- In our analysis, we aim to find a good, or good enough, model of the population.


## Model evaluation via likelihood

- Tests if the data are compatible with the model: calculate the probability of the data according to the model.
- If the probability of observing the data is higher in model $\mathcal{M}_0$ than in another $\mathcal{M}_1$, which model is more compatible with the data?
- The probability of the data according to the model is the model's *likelihood*.


## Probabilistic model 

> A possible model of each observed outcome $y_i$ is 

$$
y_i \sim N(\mu, \sigma^2)\\
\text{for } i \in 1\dots n,
$$

> We are modelling $y$ as normally distributed with two unknowns, the mean $\mu$ and standard deviation $\sigma$.


## Likelihood 

> Probability of the observed values, $y_1, y_2, y_3\dots y_n$, assuming values for $\mu$ and $\sigma$ (based on a particular model).


$$
P(y_1\dots y_n\mid\mu, \sigma)
$$

> We assume that all $y$s are conditionally independent of one another, so the joint probability is their product:

$$
P(y_1\dots y_n\mid\mu, \sigma) = \prod_\text{i=1}^n P(y_i\mid \mu, \sigma).
$$

## Likelihood

> Replace $\mu$ and $\sigma$ with their *maximum likelihood estimates* $\hat\mu$ and $\hat\sigma$:

$$
P(y_1\dots y_n\mid\hat\mu, \hat\sigma) = \prod_\text{i=1}^n P(y_i\mid \hat\mu, \hat\sigma).
$$

> Product will be a very small number (i.e. numeric underflow) cause probabilities are $0 > p < 1$, so we use logarithm:


$$
\text{log} \left( P(y_1\dots y_n\mid\hat\mu, \hat\sigma) \right) = \sum_\text{i=1}^n \text{log } P(y_i\mid \hat\mu, \hat\sigma),
$$

> Logarithm is turning product $\prod$ into sum $\sum$.
> Cause `log(.5) + log(.2)` is the same as `log(.5 * .2)`.

## Likelihood

```{r}
model_0 <- lm(log_rt ~ 1, data = blomkvist)
mu_hat <- coef(model_0) # MLE of mu
sigma_hat <- sigma(model_0) # MLE of sigma
```

```{r}
# log probability of an observed outcome for given MLEs.
logp <- dnorm(blomkvist$log_rt, mean = mu_hat, sd = sigma_hat, log = TRUE)
sum(logp) # sum of log probabilities is the log likelihood
```

```{r}
logLik(model_0)
```

> Work through exercise script: `exercises/log_likelihood_1.R`


## Likelihood

<div style="float: left; width: 35%;">

```{r}
select(blomkvist, id, log_rt, sex)
```

</div>

<div style="float: right; width: 60%;">

> Using the `sex` variable, the potential model of the log rt data is

$$
\begin{aligned}
\text{logrt}_i &\sim N(\mu_i, \sigma^2),\text{ for } i \in 1\dots n,\\
\mu_i&= \beta_0 + \beta_\text{sex} \cdot \text{sex}_i,
\end{aligned}
$$

> We are modelling log rt as normally distributed as a function of `sex`, and with an equal variance $\sigma^2$.


## Likelihood

> Give $\hat\beta_0,\hat\beta_\text{sex},\hat\sigma$, we can calculate the probability of the observed values: 

$$
P(\text{logrt}_1\dots \text{logrt}_n\mid \text{sex}_1\dots \text{sex}_n,\beta_0,\beta_\text{sex},\sigma) = \prod_\text{i=1}^n P(\text{logrt}_i\mid \text{sex}_i,\beta_0,\beta_\text{sex},\sigma)
$$

> The log likelihood of the model is


$$
\sum_\text{i=1}^n\text{ log} P(\text{logrt}_i\mid \text{sex}_i,\beta_0,\beta_\text{sex},\sigma).
$$


## Likelihood


```{r}
model_1 <- lm(log_rt ~ sex, data = blomkvist)
mu_hat <- predict(model_1) # mu_hat = beta_0 + beta_sex * sex
sigma_hat <- sigma(model_1)
```

```{r}
logp <- dnorm(blomkvist$log_rt, mean = mu_hat, sd = sigma_hat, log = TRUE)
sum(logp)
```

```{r}
logLik(model_1)
```

> Work through exercise script: `exercises/log_likelihood_2.R`




## Compare log likelihoods 

> Which model are the data more compatible with?

<div style="float: left; width: 45%;">

```{r}
logLik(model_0) # L_0
```

</div>

<div style="float: right; width: 45%;">

```{r}
logLik(model_1) # L_1
```

</div>

<div style="float: left; width: 100%;">

<div style="float: left; width: 45%;">

> Let's denote these log likelihoods as log $\mathcal{L}_0$ and log $\mathcal{L}_1$:

$$
\begin{aligned}
\text{log} \left(\frac{\mathcal{L}_1}{\mathcal{L}_0}\right)&=\text{log}\mathcal{L}_1-\text{log}\mathcal{L}_0\\
&=-13.3--17.6\\
&=4.3
\end{aligned}
$$




</div>

<div style="float: right; width: 45%;">

> Including `sex` appears to increase the log likelihood.

> Work through exercise script: `exercises/compare_log_lik.R`

</div>
</div>


## Calculating $R^2$

is the ratio of RSS (residual sum of squares; based on residuals) and TSS (total sum of squares) for which we need to work out the ESS (explained sum of squares):

$$
\underbrace{\sum_{i=1}^n(y_i-\bar{y})^2}_\text{TSS} = \underbrace{\sum_{i=1}^n(\hat\mu_i-\bar{y})^2}_\text{ESS} + \underbrace{\sum_{i=1}^n(y_i-\hat\mu_i)^2}_\text{RSS}
$$



<!-- ## Residuals -->

<!-- <div style="float: left; width: 45%;"> -->

<!-- ```{r echo = F, out.width="100%"} -->
<!-- set.seed(123) -->
<!-- n <- 20 -->
<!-- beta_0 <- 50 -->
<!-- beta_1 <- 6 -->
<!-- sigma <- 3 -->
<!-- sim <- tibble(id = 1:6) %>% -->
<!--     mutate(x = 0:5, -->
<!--            e = rnorm(n(), mean = 0, sd = sigma)) %>% -->
<!--     mutate(y = beta_0 + beta_1 * x + e) %>% -->
<!--   select(id, x, y)  -->

<!-- fake_model <- lm(y ~ x, data = sim) -->

<!-- ggplot(data =sim, aes(x = x, y = predict(fake_model))) + -->
<!--   geom_point(aes(y = y), size = 4) + -->
<!--   geom_line(colour = "red") + -->
<!--   labs(subtitle = bquote(beta[0] == .(beta_0)*","~beta[1] == .(beta_1)*","~sigma^2 == .(sigma))) + -->
<!--   theme(axis.title = element_blank(), -->
<!--         axis.text = element_blank(), -->
<!--         axis.ticks = element_blank()) -->
<!-- ``` -->

<!-- </div> -->



<!-- <div style="float: right; width: 45%;"> -->

<!-- ```{r echo = F, out.width="100%"} -->
<!-- set.seed(123) -->
<!-- n <- 20 -->
<!-- beta_0 <- 50 -->
<!-- beta_1 <- 6 -->
<!-- sigma <- 20 -->
<!-- sim <- tibble(id = 1:6) %>% -->
<!--     mutate(x = 0:5, -->
<!--            e = rnorm(n(), mean = 0, sd = sigma)) %>% -->
<!--     mutate(y = beta_0 + beta_1 * x + e) %>% -->
<!--   select(id, x, y)  -->

<!-- fake_model <- lm(y ~ x, data = sim) -->

<!-- ggplot(data = sim, aes(x = x, y = predict(fake_model))) + -->
<!--   geom_point(aes(y = y), size = 4) + -->
<!--   geom_line(colour = "red") + -->
<!--   labs(subtitle = bquote(beta[0] == .(beta_0)*","~beta[1] == .(beta_1)*","~sigma^2 == .(sigma))) + -->
<!--   theme(axis.title = element_blank(), -->
<!--         axis.text = element_blank(), -->
<!--         axis.ticks = element_blank()) -->
<!-- ``` -->

<!-- </div> -->

<!-- <div style="float: left; width: 100%;"> -->

<!-- >- Which model is better and why? -->
<!-- >- How can we reduce the residual error? -->

<!-- </div> -->


<!-- ## Residuals -->

<!-- <div style="float: left; width: 45%;"> -->

<!-- ```{r echo = F, out.width="100%"} -->
<!-- set.seed(123) -->
<!-- n <- 20 -->
<!-- beta_0 <- 50 -->
<!-- beta_1 <- 6 -->
<!-- sigma <- 3 -->
<!-- sim <- tibble(id = 1:6) %>% -->
<!--     mutate(x = 0:5, -->
<!--            e = rnorm(n(), mean = 0, sd = sigma)) %>% -->
<!--     mutate(y = beta_0 + beta_1 * x + e) %>% -->
<!--   select(id, x, y)  -->

<!-- fake_model <- lm(y ~ x, data = sim) -->

<!-- ggplot(data =sim, aes(x = x, y = predict(fake_model))) + -->
<!--   geom_point(aes(y = y), size = 4) + -->
<!--   geom_line(colour = "red") + -->
<!--   labs(subtitle = bquote(beta[0] == .(beta_0)*","~beta[1] == .(beta_1)*","~sigma^2 == .(sigma))) + -->
<!--   geom_segment( aes(xend = x, yend = y), -->
<!--       size = 0.5, alpha = 0.5, lineend = "round") + -->
<!--   theme(axis.title = element_blank(), -->
<!--         axis.text = element_blank(), -->
<!--         axis.ticks = element_blank()) -->
<!-- ``` -->

<!-- </div> -->



<!-- <div style="float: right; width: 45%;"> -->

<!-- ```{r echo = F, out.width="100%"} -->
<!-- set.seed(123) -->
<!-- n <- 20 -->
<!-- beta_0 <- 50 -->
<!-- beta_1 <- 6 -->
<!-- sigma <- 20 -->
<!-- sim <- tibble(id = 1:6) %>% -->
<!--     mutate(x = 0:5, -->
<!--            e = rnorm(n(), mean = 0, sd = sigma)) %>% -->
<!--     mutate(y = beta_0 + beta_1 * x + e) %>% -->
<!--   select(id, x, y)  -->

<!-- fake_model <- lm(y ~ x, data = sim) -->

<!-- ggplot(data = sim, aes(x = x, y = predict(fake_model))) + -->
<!--   geom_point(aes(y = y), size = 4) + -->
<!--   geom_line(colour = "red") + -->
<!--   labs(subtitle = bquote(beta[0] == .(beta_0)*","~beta[1] == .(beta_1)*","~sigma^2 == .(sigma))) + -->
<!--   geom_segment( aes(xend = x, yend = y), -->
<!--       size = 0.5, alpha = 0.5, lineend = "round") + -->
<!--   theme(axis.title = element_blank(), -->
<!--         axis.text = element_blank(), -->
<!--         axis.ticks = element_blank()) -->
<!-- ``` -->

<!-- </div> -->

<!-- <div style="float: left; width: 100%;"> -->

<!-- >- Which model is better and why? -->
<!-- >- How can we reduce the residual error? -->

<!-- </div> -->


## Residuals


<div style="float: left; width: 50%;">

```{r out.width="100%", fig.height=7.25, echo = F}


model <- lm(log_rt ~ age, data = blomkvist)
blomkvist$pred <- predict(model)
blomkvist$resid <- residuals(model)

plot_1 <- ggplot(blomkvist, aes(x = age, y = pred) ) +
  geom_line(size = 1) +
  scale_colour_viridis_d(end = .6) +
  labs(y = bquote("Intercept\nmodel M"[1])) +
  geom_point(aes(x = age, y = log(rt)), size = 1) +
  theme(axis.title.x = element_blank())

model <- lm(log_rt ~ age + sex, data = blomkvist)
blomkvist$pred <- predict(model)
blomkvist$resid <- residuals(model)

plot_2 <- ggplot(blomkvist, aes(x = age, y = pred, colour = sex) ) +
  geom_line(size = 1) +
  scale_colour_viridis_d(end = .6) +
  labs(y = bquote("VI model M"[2])) +
  geom_point(aes(x = age, y = log(rt), colour = sex), size = 1) +
  theme(axis.title.x = element_blank())

model <- lm(log_rt ~ age * sex, data = blomkvist)
blomkvist$pred <- predict(model)
blomkvist$resid <- residuals(model)

plot_3 <- ggplot(blomkvist, aes(x = age, y = pred, colour = sex) ) +
  geom_line(size = 1) +
  scale_colour_viridis_d(end = .6) +
  labs(y = bquote("VI and VS\nmodel M"[3])) +
  geom_point(aes(x = age, y = log(rt), colour = sex), size = 1)

plot <- plot_1 / plot_2 / plot_3 +
  plot_layout(guides = "collect");plot
```
</div>


## Residuals

<div style="float: left; width: 50%;">

```{r out.width="100%", fig.height=7.25, echo = F}

plot_1 <- plot_1 + geom_segment(
      aes(xend = age, yend = log(rt)),
      size = 0.5, alpha = 0.5, lineend = "round")

plot_2 <- plot_2 + geom_segment(
      aes(xend = age, yend = log(rt)),
      size = 0.5, alpha = 0.5, lineend = "round")

plot_3 <- plot_3 + geom_segment(
      aes(xend = age, yend = log(rt)),
      size = 0.5, alpha = 0.5, lineend = "round")

plot <- plot_1 / plot_2 / plot_3 +
  plot_layout(guides = "collect");plot
```
</div>

<div style="float: right; width: 45%;">

>- Residuals are the unexplained (residual) variance: error in the modelling results.
>- Distance between observed ($y$) and predicted rt ($\hat{\mu}$)

$$
\epsilon = y - \hat{\mu}
$$

>- The closer the residuals are to 0, the lower the prediction error.

</div>


## Residuals

<div style="float: left; width: 50%;">

```{r out.width="100%", fig.height=7.25, echo = F}
plot
```
</div>

<div style="float: right; width: 45%;">

```{r}
mutate(blomkvist, 
       mu_hat = predict(model),
       resid_1 = log_rt - mu_hat,
       resid_2 = residuals(model)) %>% 
  select(log_rt, mu_hat, resid_1, resid_2)  
```

Work through exercise script: `exercises/residuals.R`


</div>






## Residual sum of squares 

The sum of squared residuals when using the maximum likelihood estimators is

$$
\begin{aligned}
\text{RSS}&=\sum_\text{i=1}^n \mid y_i - (\hat\beta_0+\hat\beta_1\cdot x_i)\mid^2,\\
&=\sum_\text{i=1}^n\mid y_i-\hat{\mu}_i\mid^2
\end{aligned}
$$

RSS is a measure of how much variance in the data is not explained by the model; or the model's lack of fit.


## Residual sum of squares



```{r}
# RSS: total of unexplained variance
(rss <- sum(residuals(model)^2))
```


```{r echo = F, out.width="100%", fig.height=2.25}
resid <- as.vector(residuals(model))
y <- blomkvist$log_rt
#as.vector(predict(model_0, newdata = blomkvist))

y_plot <- ggplot(data = NULL, aes(x = y)) +
  geom_histogram() +
  labs(x = "y") +
  theme_bw(base_size = 11) +
  theme(axis.title.y = element_blank())

resid_plot <- ggplot(data = NULL, aes(x = resid)) +
  geom_histogram() +
  theme_bw(base_size = 11) +
  labs(x = bquote("residuals" == "y" - hat(mu))) +
  theme(axis.title.y = element_blank())

resid_sqrt_plot <- ggplot(data = NULL, aes(x = resid^2)) +
  geom_histogram() +
  theme_bw(base_size = 11) +
  labs(x = bquote("residuals"^2)) +
  theme(axis.title.y = element_blank())

y_plot + resid_plot + resid_sqrt_plot
```



<!-- ## Residual sum of squares and log likelihood  -->


<!-- > Knowing the RSS allows you to calculae the log-likelihood. -->

<!-- $$ -->
<!-- \text{log}(\mathcal{L})=-\frac{n}{2}\left( \text{log}(2\pi)-\text{log}(n) + \text{log(RSS)}+1 \right) -->
<!-- $$ -->

<!-- > Log likelihood is calculated on the basis of RSS and the number of observations $n$. -->


<!-- <div style="float: left; width: 55%;"> -->

<!-- ```{r} -->
<!-- n <- nrow(blomkvist) -->
<!-- -(n/2) * (log(2*pi) - log(n) + log(rss) + 1) -->
<!-- ``` -->

<!-- </div> -->

<!-- <div style="float: right; width: 40%;"> -->
<!-- ```{r} -->
<!-- logLik(model) -->
<!-- ``` -->
<!-- </div> -->




## Explained sum of squares


<div style="float: left; width: 50%;">

ESS is sum of squared differences of predicted data and sample mean.

$$
\text{ESS} = \sum_{i=1}^n(\hat\mu_i-\bar{y})^2
$$


Predicted mean value $\hat\mu_i$ for each observation $i$:

$$
\hat\mu_i = \hat\beta_0 + \hat\beta_1 \cdot x_{i}$
$$

</div>

<div style="float: right; width: 45%;">

```{r}
(mu_hat <- predict(model))[1:10]
```

$\bar{y}$ is the mean of the sample.

```{r}
(y_bar <- mean(blomkvist$log_rt))
```


```{r}
# ESS: explained sum of squares
(ess <- sum( (mu_hat - y_bar)^2 ))
```

</div>



## $R^2$: coefficient of determination 

calculated on the basis of TSS which is the sum of ESS and RSS

$$
\underbrace{\sum_{i=1}^n(y_i-\bar{y})^2}_\text{TSS} = \underbrace{\sum_{i=1}^n(\hat\mu_i-\bar{y})^2}_\text{ESS} + \underbrace{\sum_{i=1}^n(y_i-\hat\mu_i)^2}_\text{RSS}
$$

```{r}
(tss <- ess + rss)
```

## $R^2$: coefficient of determination 

>- Ability of a model to predict the outcome variable.
>- Proportion of the variability in the outcome attributed to the predictor(s).
>- Routinely used as measure of model fit.


<div style="float: left; width: 60%;">

$$
R^2 = \frac{\text{ESS}}{\text{TSS}}
$$

> Work through script: `exercises/rsquared.R`

</div>

<div style="float: right; width: 30%;">

```{r}
ess / tss
```


```{r}
summary(model)$r.sq
```

</div>




<!-- ## Root mean square error  -->

<!-- <div style="float: left; width: 45%;"> -->

<!-- $$ -->
<!-- \text{RMSE} = \sqrt{\frac{\text{RSS}}{n}} -->
<!-- $$ -->

<!-- >- Alternative to RSS is the square root of the mean squared residuals. -->
<!-- >- Doesn't highlight explained but unexplained variance. -->
<!-- >- Larger the sample, larger RSS. -->
<!-- >- Average distance between model fit and data, while taking into account sample size. -->


<!-- </div> -->

<!-- <div style="float: right; width: 45%;"> -->

<!-- ```{r} -->
<!-- n <- nrow(blomkvist) -->
<!-- rss <- sum(residuals(model)^2) -->
<!-- sqrt(rss/n) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- sqrt(mean(residuals(model)^2)) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- sd(residuals(model)) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- sigma(model) -->
<!-- ``` -->

<!-- </div> -->


<!-- ## Mean absolute error  -->

<!-- Related to the root mean squared error is the mean absolute error (MAE), which is the mean of the absolute values of the residuals. -->

<!-- $$ -->
<!-- \text{MAE} = \frac{\sum_\text{i=1}^n\mid y_i-\hat{\mu}_1\mid}{n} -->
<!-- $$ -->

<!-- ```{r} -->
<!-- sum(abs(residuals(model))) / n -->
<!-- ``` -->


<!-- ```{r} -->
<!-- mean(abs(residuals(model))) -->
<!-- ``` -->


<!-- Check out exercise `exercises/rmse_mae.R` -->



## Adjusted $R^2$ 

<div style="float: left; width: 45%;">

- The value of $R^2$ necessarily increases, or does not decrease, as we add more predictors to the model, even if the true values of the coefficients for these predictors are zero.
- Overfitting: Complex models explain more variance but may make over optimistic predictions [@gelman2020regression].
- Example: brain size and body mass for seven primate species [from @mcelreath2016statistical].

</div>

<div style="float: right; width: 45%;">

```{r fig.height=6.5, out.width="100%", echo = F}
d <- tibble(species = c("afarensis", "africanus", "habilis", "boisei", "rudolfensis", "ergaster", "sapiens"), 
         brain   = c(438, 452, 612, 521, 752, 871, 1350), 
         mass    = c(37.0, 35.5, 34.5, 41.5, 55.5, 61.0, 53.5))

fit_lm <- function(model, formula){
  model <- lm(data = d, formula = formula)
}

fits <- tibble(model   = str_c("b6.", 1:6),
         formula = c("brain ~ mass", 
                     "brain ~ mass + I(mass^2)", 
                     "brain ~ mass + I(mass^2) + I(mass^3)", 
                     "brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4)", 
                     "brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(mass^5)", 
                     "brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(mass^5) + I(mass^6)")) %>% 
  mutate(fit     = map2(model, formula, fit_lm)) %>% 
  mutate(tidy    = map(fit, tidy),
         glance  = map(fit, glance))

fits <- fits %>% 
  mutate(adjr2      = glance %>% map_dbl("adj.r.squared")) %>% 
  mutate(r2      = glance %>% map_dbl("r.squared")) %>% 
  mutate(r2_text = round(r2, digits = 2) %>% as.character() %>% str_replace(., "0.", "."))

ggplot(d, aes(x = mass, y = brain, label = species)) +
#  geom_point(size = 2.5, shape = 21) +
  geom_text() +
  scale_x_continuous(limits = c(33, 62), expand = c(0, 0)) +
  coord_cartesian(ylim = c(300, 1500)) +
  stat_smooth(method = "lm", fullrange = TRUE, level = .89,
              size = 1/2, alpha = 1/3, colour = "darkred",
              formula = y ~ x) +
  labs(title = "Linear model", subtitle = expression(paste(italic(R)^2, " = .49")),
       y  = "Brain volume (cc)", x = "Body mass (kg)")
```

</div>



## Adjusted $R^2$ 

```{r echo = F, out.width="90%", fig.width=12}
d <- tibble(species = c("afarensis", "africanus", "habilis", "boisei", "rudolfensis", "ergaster", "sapiens"), 
         brain   = c(438, 452, 612, 521, 752, 871, 1350), 
         mass    = c(37.0, 35.5, 34.5, 41.5, 55.5, 61.0, 53.5))

fit_lm <- function(model, formula){
  model <- lm(data = d, formula = formula)
}

fits <- tibble(model   = str_c("b6.", 1:6),
         formula = c("brain ~ mass", 
                     "brain ~ mass + I(mass^2)", 
                     "brain ~ mass + I(mass^2) + I(mass^3)", 
                     "brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4)", 
                     "brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(mass^5)", 
                     "brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(mass^5) + I(mass^6)")) %>% 
  mutate(fit     = map2(model, formula, fit_lm)) %>% 
  mutate(tidy    = map(fit, tidy),
         glance  = map(fit, glance))


fits <- fits %>% 
  mutate(r2      = glance %>% map_dbl("r.squared")) %>% 
  mutate(r2_text = round(r2, digits = 2) %>% as.character() %>% str_replace(., "0.", "."))

p <-  ggplot(d, aes(x = mass, y = brain, label = species)) +
  geom_point(size = 2.5, shape = 21) +
  scale_x_continuous(limits = c(33, 62), expand = c(0, 0)) +
  coord_cartesian(ylim = c(300, 1500)) +
  theme(axis.title = element_blank(),
        plot.title = element_text(size = 14),
        plot.subtitle = element_text(size = 12))

# linear
p1 <-  p + stat_smooth(method = "lm", fullrange = TRUE, level = .89,
              size = 1/2, alpha = 1/3, colour = "darkred",
              formula = y ~ x) +
  labs(title = "Linear model", subtitle = expression(paste(italic(R)^2, " = .49")))
  
# cubic
p3 <- p + stat_smooth(method = "lm", fullrange = TRUE, level = .89,
              size = 1/2, alpha = 1/3,  colour = "darkred",
              formula = y ~ poly(x, 3)) +
  labs(title = "3rd-order polynomial", subtitle = expression(paste(italic(R)^2, " = .68")))

# sixth-order polynomial
p6 <-  p + geom_hline(yintercept = 0, linetype = 2) + 
  stat_smooth(method = "lm", fullrange = TRUE, level = .89,
              size = 1/2, alpha = 1/3,  colour = "darkred",
              formula = y ~ poly(x, 6)) +
  coord_cartesian(ylim = c(-300, 1500)) +
  labs(title = "6th-order polynomial", subtitle = expression(paste(italic(R)^2, " = 1")))

plot <- p1 + p3 + p6

gt <- patchwork::patchworkGrob(plot)
gridExtra::grid.arrange(gt, left = "Brain volume (cc)", bottom = "Body mass (kg)")
```



## Adjusted $R^2$

To overcome this spurious increase in $R^2$, the following adjustment is applied.

$$
\begin{aligned}
R^2_\text{Adj} &= R^2 \cdot \underbrace{\frac{n-1}{n-K-1}}_{\text{penalty}}\\
\end{aligned}
$$


## Adjusted $R^2$ 

```{r echo = F, out.width="90%", fig.width=12}
d <- tibble(species = c("afarensis", "africanus", "habilis", "boisei", "rudolfensis", "ergaster", "sapiens"), 
         brain   = c(438, 452, 612, 521, 752, 871, 1350), 
         mass    = c(37.0, 35.5, 34.5, 41.5, 55.5, 61.0, 53.5))

fit_lm <- function(model, formula){
  model <- lm(data = d, formula = formula)
}

fits <- tibble(model   = str_c("b6.", 1:6),
         formula = c("brain ~ mass", 
                     "brain ~ mass + I(mass^2)", 
                     "brain ~ mass + I(mass^2) + I(mass^3)", 
                     "brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4)", 
                     "brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(mass^5)", 
                     "brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(mass^5) + I(mass^6)")) %>% 
  mutate(fit     = map2(model, formula, fit_lm)) %>% 
  mutate(tidy    = map(fit, tidy),
         glance  = map(fit, glance)) %>% 
  mutate(r2      = glance %>% map_dbl("adj.r.squared"))  

p <-  ggplot(d, aes(x = mass, y = brain, label = species)) +
  geom_point(size = 2.5, shape = 21) +
  scale_x_continuous(limits = c(33, 62), expand = c(0, 0)) +
  coord_cartesian(ylim = c(300, 1500)) +
  theme(axis.title = element_blank(),
        plot.title = element_text(size = 14),
        plot.subtitle = element_text(size = 12))

# linear
p1 <-  p + stat_smooth(method = "lm", fullrange = TRUE, level = .89,
              size = 1/2, alpha = 1/3, colour = "darkred",
              formula = y ~ x) +
  labs(title = "Linear model", subtitle = expression(paste(italic(R)["Adj"]^2, " = .39")))
  
# cubic
p3 <- p + stat_smooth(method = "lm", fullrange = TRUE, level = .89,
              size = 1/2, alpha = 1/3,  colour = "darkred",
              formula = y ~ poly(x, 3)) +
  labs(title = "3rd-order polynomial", subtitle = expression(paste(italic(R)["Adj"]^2, " = .36")))

# sixth-order polynomial
p6 <-  p + geom_hline(yintercept = 0, linetype = 2) + 
  stat_smooth(method = "lm", fullrange = TRUE, level = .89,
              size = 1/2, alpha = 1/3,  colour = "darkred",
              formula = y ~ poly(x, 6)) +
  coord_cartesian(ylim = c(-300, 1500)) +
  labs(title = "6th-order polynomial", subtitle = expression(paste(italic(R)["Adj"]^2, " = NaN")))

plot <- p1 + p3 + p6

gt <- patchwork::patchworkGrob(plot)
gridExtra::grid.arrange(gt, left = "Brain volume (cc)", bottom = "Body mass (kg)")
```




## $R^2$ and Adjusted $R^2$ with `lm` 


<div style="float: left; width: 55%;">

```{r}
# Specify models
model_1 <- lm(log_rt ~ sex, data = blomkvist)
model_2 <- lm(log_rt ~ sex + age, data = blomkvist)
```

From the `lm` objects, $R^2$ and $R_\text{Adj}^2$ can be obtained using `summary`

```{r}
# Generate model summary
summary_model_1 <- summary(model_1)
summary_model_2 <- summary(model_2)
```

Because $n \gg K$, the penalty term is (very) close to 1.0 and the adjustment is minimal.

Do exercise `exercises/adjrsquared.R`


</div>

<div style="float: right; width: 35%;">

```{r}
summary_model_1$r.squared
```

```{r}
summary_model_2$r.squared
```

```{r}
summary_model_1$adj.r.squared
```

```{r}
summary_model_2$adj.r.squared
```

</div>




## Model comparison

- $R^2=0$ is only true if $\beta_1=\beta_2=\dots\beta_k=0$.
- When all coefficients are simultaneously zero, we are essentially saying that the following two models are identical.

$$
\begin{aligned}
\mathcal{M}_0:& y_i \sim N(\hat\mu, \sigma^2), \hat\mu_i = \beta_0\\
\mathcal{M}_1:& y_i \sim N(\hat\mu, \sigma^2), \hat\mu_i = \beta_0 + \sum_{k=1}^K \beta_k \cdot x_{ki}
\end{aligned}
$$

> $\text{ for } i \in 1\dots n$

## Model comparison

>- Null hypothesis is that the following two models are identical:

$$
\begin{aligned}
\mathcal{M}_0:& \text{logrt}_i \sim N(\hat\mu, \sigma^2), \hat\mu_i = \beta_0\\
\mathcal{M}_1:& \text{logrt}_i \sim N(\hat\mu, \sigma^2), \hat\mu_i = \beta_0 + \beta_\text{sex} \cdot \text{sex}_{i}
\end{aligned}
$$

> $\text{ for } i \in 1\dots n$


>- In other words, the null hypothesis is that adding the predictor $\text{sex}$ in $\mathcal{M}_1$ does not explain any variance in the data that model $\mathcal{M}_0$ was already accounting for.


## Model comparison 

>- Nested normal linear models can be compared using F test.
>- Under the null hypothesis that these two models are identical, and so the coefficients for all predictors are simultaneously zero, we have 

$$
\underbrace{\frac{(\text{RSS}_0-\text{RSS}_1)/K}{\text{RSS}_1/(n - K - 1)}}_\text{F-statistic} \sim F(K, n-K-1)
$$

- We can compare two models $\mathcal{M}_1$ and $\mathcal{M}_0$ (with $K_1$ and $K_0$ predictors, respectively) where $\mathcal{M}_0$ is *nested* in $\mathcal{M}_1$:
  - $K_0 < K_1$ 
  - all the $K_0$ predictors in $\mathcal{M}_0$ are also presented in $\mathcal{M}_1$

## F distribution 


<div style="float: left; width: 45%;">
```{r}
df1 <- seq(1, 21, 3)
df2 <- 300  
F_grid <- seq(0, 5, .1) 
```

```{r echo = F, out.width="100%"}
# Increase df1 as for more complex model
data_df1 <- map_dfr(df1, ~pf(q = F_grid, df1 = .x, df2 = df2, lower.tail = F) %>% tibble(F_grid = F_grid, p = .) %>%  
    mutate(df1 = .x)) 

ggplot(data = data_df1, aes(x = F_grid, y = p, colour = factor(df1))) +
  geom_line() +
  labs(x = "Theoretical F value",
       y = "p value",
       colour = bquote("df"[1])) +
  scale_colour_viridis_d() +
  geom_hline(yintercept = 0.05, linetype = "dotted")

```

</div>

<div style="float: right; width: 45%;">

```{r}
df1 <- 5
df2 <- seq(3, 300, 30)
F_grid <- seq(0, 5, .1) 
```

```{r echo = F, out.width="100%"}
# Increase df1 as for more complex model
data_df2 <- map_dfr(df2, ~pf(q = F_grid, df1 = df1, df2 = .x, lower.tail = F) %>% tibble(F_grid = F_grid, p = .) %>%  
    mutate(df2 = .x)) 

ggplot(data = data_df2, aes(x = F_grid, y = p, colour = factor(df2))) +
  geom_line() +
  labs(x = "Theoretical F value",
       y = "p value",
       colour = bquote("df"[2])) +
  scale_colour_viridis_d() +
  geom_hline(yintercept = 0.05, linetype = "dotted")

```

</div>




<!-- ## Nested models  -->

<!-- - We can test whether any subset of the $K$ predictors have coefficients that are simultaneously zero. -->
<!-- - Under the null hypothesis that the $K_1-K_0$ predictors in $\mathcal{M}_1$ and not in $\mathcal{M}_0$ are simultaneously zero, we have -->

<!-- $$ -->
<!-- \frac{(\text{RSS}_0-\text{RSS}_1)/(K_1-K_0)}{\text{RSS}_1/(n - K_1 - 1)} \sim F(K_1 - K_0, n-K_1-1). -->
<!-- $$ -->
<!-- - For a $\mathcal{M}_0$ with $K_0=1$ nested within $\mathcal{M}_1$ with $K_1=2$ (for example the predictor $\text{sex}$) this renders: -->


<!-- $$ -->
<!-- \frac{(\text{RSS}_0-\text{RSS}_1)}{\text{RSS}_1/n-1} \sim F(1, n-1). -->
<!-- $$ -->


<!-- where  -->

<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \text{proportional increase in error} &= \frac{\text{RSS}_0-\text{RSS}_1}{\text{RSS}_1}\\ -->
<!-- &= \frac{\text{change in error (from } \mathcal{M}_0\text{ to }\mathcal{M}_1)}{\text{minimal error}}\\ -->
<!-- \end{aligned} -->
<!-- $$ -->

## Model comparison 

> The null hypothesis that $R^2=0$ can be obtained using `anova` to compare `model_1` against `model_0`.  

<div style="float: left; width: 47%;">

```{r}
# Specify models
model_0 <- lm(log_rt ~ 1, data = blomkvist)
model_1 <- lm(log_rt ~ sex, data = blomkvist)
```

```{r eval = F}
# Compare models
anova(model_0, model_1)
```

```{r echo = F}
# Compare models
print(anova(model_0, model_1), signif.stars=F)
```


</div>

<div style="float: right; width: 50%;">


</div>


## Model comparison 

> The null hypothesis that $R^2=0$ can be obtained using `anova` to compare `model_1` against `model_0`.  

<div style="float: left; width: 47%;">

```{r}
# Specify models
model_0 <- lm(log_rt ~ 1, data = blomkvist)
model_1 <- lm(log_rt ~ sex, data = blomkvist)
```

```{r eval = F}
# Compare models
anova(model_0, model_1)
```

```{r echo = F}
# Compare models
print(anova(model_0, model_1), signif.stars=F)
```


</div>

<div style="float: right; width: 50%;">

>- `RSS` are TSS and RSS of `model_1`.
>- TSS of `model_0`: in model with no predictors, TSS and RSS are identical.
>- ESS is `Sum of Sq` / difference of `RSS`s:

```{r}
mu_hat <- predict(model_1)
y_bar <- mean(blomkvist$log_rt)
sumdiff <- mu_hat - y_bar
(ess <- sum(sumdiff^2))
```


</div>


## Model comparison

> The null hypothesis that $R^2=0$ can be obtained using `anova` to compare `model_1` against `model_0`.  

<div style="float: left; width: 47%;">

```{r}
# Specify models
model_0 <- lm(log_rt ~ 1, data = blomkvist)
model_1 <- lm(log_rt ~ sex, data = blomkvist)
```

```{r eval = F}
# Compare models
anova(model_0, model_1)
```

```{r echo = F}
# Compare models
print(anova(model_0, model_1), signif.stars=F)
```


</div>

<div style="float: right; width: 50%;">

>- `Df` and bottom value of `Res.Df` are degrees of freedom 

```{r}
K <- 1
n <- nrow(blomkvist)
c(K, n - K - 1)
```

>- `ess` and `rss` are divided for F:

```{r}
rss <- sum(residuals(model_1)^2)
(f_stat <- (ess/K) / (rss/(n - K - 1)))
```

</div>


## Model comparison 

> The null hypothesis that $R^2=0$ can be obtained using `anova` to compare `model_1` against `model_0`.  

<div style="float: left; width: 47%;">

```{r}
# Specify models
model_0 <- lm(log_rt ~ 1, data = blomkvist)
model_1 <- lm(log_rt ~ sex, data = blomkvist)
```

```{r eval = F}
# Compare models
anova(model_0, model_1)
```

```{r echo = F}
# Compare models
print(anova(model_0, model_1), signif.stars=F)
```


</div>

<div style="float: right; width: 50%;">

>- *p*-value is the probability of getting a greater / or equal F statistic in an F distribution with $K$ and $n - K - 1$ degrees of freedom.
>- Cumulative distribution function of the F distribution `pf`

```{r}
pf(f_stat, K, n-K-1, lower.tail = FALSE)
```

</div>


## F ratio

$$
\text{F} = \underbrace{\frac{\text{RSS}_0 - \text{RSS}_1}{\text{RSS}_1}}_\text{effect size} \cdot \underbrace{\frac{\text{df}_1}{\text{df}_0 -\text{df}_1}}_\text{sample size} = \frac{(\text{RSS}_0 - \text{RSS}_1)/(\text{df}_0 - \text{df}_1)}{\text{RSS}_1/\text{df}_1}
$$

where Df$_1$ is $n - K_1 + 1$, $K_1$ is the number of predictors in $\mathcal{M}_1$.

<div style="float: left; width: 45%;">

```{r}
# Dfs
df_0 <- model_0$df.residual
df_1 <- model_1$df.residual
c(df_0 - df_1, df_1)
```

```{r}
# RSS
rss_0 <- sum(residuals(model_0)^2)
rss_1 <- sum(residuals(model_1)^2)
```

</div>

<div style="float: right; width: 45%;">

```{r}
(rss_0 - rss_1)/(df_0 - df_1)
```


```{r echo = F, eval = F}
rss_1 / df_1
```


```{r}
((rss_0 - rss_1)/(df_0 - df_1))/(rss_1/df_1)

```

Complete exercise `exercises/modelcomparison.R`

</div>

## F distribution

```{r}
df1 <- K # Number of fixed effects
df2 <- n - K - 1  
F_grid <- seq(0, 10, .1) 
p_values <- pf(q = F_grid, df1 = df1, df2 = df2, lower.tail = F)
```

```{r echo = F, fig.height=3}
p <- ggplot(data = NULL, aes(x = F_grid, y = p_values)) +
  geom_line() +
  labs(x = "Theoretical F value",
       y = "p value",
       subtitle = bquote("F distribution for"~"df"[1] == .(df1)~"and df"[2] == .(df2)));p 
```


## F distribution

```{r}
df1 <- K # Number of fixed effects
df2 <- n - K - 1  
F_grid <- seq(0, 10, .1) 
p_values <- pf(q = F_grid, df1 = df1, df2 = df2, lower.tail = F)
```

```{r echo = F, fig.height=3}
label <- deparse(bquote(paste("F" == .(round(f_stat, 1)))))

p2 <- p + geom_segment(aes(x = 8.2, y = .2, xend = f_stat, yend = 0.02),
                  arrow = arrow(length = unit(0.35, "cm")),
               colour = "red") +
  geom_label(aes(x = 8, y = 0.3, label = label), parse = T); p2
```

## F distribution

```{r}
df1 <- K # Number of fixed effects
df2 <- n - K - 1  
F_grid <- seq(0, 10, .1) 
p_values <- pf(q = F_grid, df1 = df1, df2 = df2, lower.tail = F)
```

```{r echo = F, fig.height=3}
p_value <- pf(q = f_stat, df1 = df1, df2 = df2, lower.tail = F)
label_2 <- deparse(bquote(paste("p" == .(round(p_value, 3)))))

p3 <- p2 + geom_hline(yintercept = p_value, colour = "red", linetype = "dashed") +
  geom_label(aes(x = .65, y = 0.1, label = label_2), parse = T);p3
```


## F distribution

```{r}
df1 <- K # Number of fixed effects
df2 <- n - K - 1  
F_grid <- seq(0, 10, .1) 
p_values <- pf(q = F_grid, df1 = df1, df2 = df2, lower.tail = F)
```

```{r echo = F, fig.height=3}
p3 + annotate("label", x = 8, y = .8, label = "Work through script\n exercise/f_distribution.R")

```





## Deviance information criterion (DIC)

$$
\text{DIC} = -2 \cdot \text{log} \hat{\mathcal{L}}
$$

where $\hat{\mathcal{L}}$ is the likelihood of the model using the MLEs.

>- *Lower* deviance indicates better model fit
>- Equivalent to RSS for generalised linear models.

```{r}
deviance(model_1)
```


```{r}
rss_1
```



## Akaike and Bayesian Information Criterion 

<div style="float: left; width: 55%;">

>- Estimators of prediction error: relative quality of statistical models for data.
>- Similar to $R^2$, log likelihood will always increase when adding predictors, even if the true value of the coefficient is zero.
>- Prevent overfitting by penalising models with more parameters: trade-off model fit (measured by $\hat{\mathcal{L}}$) and the number of parameters ($K$) in the model.


$$
\text{AIC} = 2 \cdot K - 2 \cdot \text{log}(\hat{\mathcal{L}})\\
\text{BIC} = \text{log}(n) \cdot K - 2\cdot \text{log}(\hat{\mathcal{L}})\\
$$


</div>

<div style="float: right; width: 35%;">

```{r}
ll <- as.vector(logLik(model_1))
K <- 3 # beta_0, beta_sex, sigma^2
(2 * K) - 2 * ll # AIC
```

```{r}
AIC(model_1)
```

```{r}
n <- nrow(blomkvist) 
log(n) * K - 2 * ll # BIC
```

```{r}
BIC(model_1)
```

</div>




## Akaike and Bayesian Information Criterion 

<div style="float: left; width: 55%;">

>- Estimators of prediction error: relative quality of statistical models for data.
>- Similar to $R^2$, log likelihood will always increase when adding predictors, even if the true value of the coefficient is zero.
>- Prevent overfitting by penalising models with more parameters: trade-off model fit (measured by $\hat{\mathcal{L}}$) and the number of parameters ($K$) in the model.


$$
\text{AIC} = 2 \cdot K - 2 \cdot \text{log}(\hat{\mathcal{L}})\\
\text{BIC} = \text{log}(n) \cdot K - 2\cdot \text{log}(\hat{\mathcal{L}})\\
$$


</div>


<div style="float: right; width: 43%;">

- AIC chooses models with better predictive ability; it tends to favour bigger models as the sample size increases.
- BIC is an approximation to Bayesian model comparison by Bayes factors. 
- In many situations collecting more data will uncover more complexity in the process generating the data, in which case AIC may be more suitable. 

</div>



## Comparing multiple nested models 

... including the varying-intercepts (`model_2`) and the varying-intercepts and varying-slopes model (`model_3`).

<div style="float: left; width: 52%;">

```{r}
# Specify models
model_0 <- lm(log_rt ~ 1, data = blomkvist)
model_1 <- lm(log_rt ~ sex, data = blomkvist)
model_2 <- lm(log_rt ~ sex + age, data = blomkvist)
model_3 <- lm(log_rt ~ sex * age, data = blomkvist)
```

```{r eval = F}
# Compare models
anova(model_0, model_1, model_2, model_3)
```

</div>

<div style="float: right; width: 43%;">


```{r echo = F}
# Compare models
print(anova(model_0, model_1, model_2, model_3), signif.stars=F)
```
</div>


## Model evaluation made easy 


```{r eval = F}
# Drop highest level predictor 
drop1(model_3, scope = ~ sex * age, test = 'F')
```

```{r echo = F}
# Compare models
print(drop1(model_3, scope = ~ sex * age, test = 'F'), signif.stars=F)
```

```{r}
# All you ever need :)
library(broom)
#library(broom.mixed) # for multilevel models
```


```{r}
tidy(model_3, conf.int = TRUE)
```

```{r}
glance(model_3) %>% glimpse()
```

```{r}
augment(model_3)
```

Check `exercises/comparing_multiple_nested_models.R`


## Watch out

>- Rejecting the null hypothesis in favour of an alternative model does not mean this model is the best (i.e. it is better than the baseline model).
>- Failing to reject the null hypothesis in favour of an alternative model does not mean that the null model is correct or that an additional coefficient is not theoretically relevant.



## The end

>- Comparing models with nested sets of predictors.
>- Evaluation is based on likelihood of data under the model.
>- Penalising more complex models by considering number of parameters.
>- Other comparison techniques for models that are not nested within each other: e.g. different probability models (e.g. Poisson vs negative binomial).
>- Cross-validation (e.g. "LOO": Leave-one-out CV, k-fold CV, Monte Carlo CV).



```{r eval=FALSE, echo=FALSE}

https://cran.r-project.org/doc/contrib/Faraway-PRA.pdf
Chapter 3 Faraway 
```


## References {.smaller}

<style>
slides > slide { overflow: scroll; }
slides > slide:not(.nobackground):after {
  content: '';
}
</style>


