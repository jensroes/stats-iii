---
title: 'Model comparison'
author: "Jens Roeser"
output: 
  ioslides_presentation:
    incremental: false
    transition: slower
    widescreen: true
    css: ../slides.css
    logo: ../gfx/ntu.png
bibliography      : ["../references.bib"]
---

```{=html}
<style>

slides > slide.backdrop {
  background: white;
}

.gdbar img {
  width: 240px !important;
  height: 54px !important;
  margin: 8px 8px;
  position: absolute;
}

.gdbar {
  width: 350px !important;
  height: 70px !important;
}

slides > slide:not(.nobackground):before {
  width: 128px;
  height: 33px;
  background-size: 100px 30px;
}

.smalltable .table{
  font-size: 18px;
}

pre {
  font-size: 15px;
}
</style>
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, eval = TRUE, message = FALSE, comment=NA)
options("kableExtra.html.bsTable" = T, digits = 3)
options(pillar.print_min = 5, pillar.print_max = 6)
library(tidyverse)
library(knitr)
library(kableExtra)
library(patchwork)
library(broom)
theme_set(theme_bw(base_size = 18) +
            theme(legend.position = "top", 
                  legend.justification = "right",
                  panel.grid = element_blank()))
```


## Normal linear model {.smaller}

<div style="float: left; width: 35%;">

$$
y_i \sim N(\mu_i, \sigma^2)\\
\mu_i = \beta_0 + \sum_{k=1}^K \beta_k \cdot x_{ki}\\
\text{for } i \in 1\dots n
$$
</div>

<div style="float: left; width: 65%;">

- Outcome variable is normally distributed.
- Distribution can be characterised with an unknown mean $\mu_i$ and variance $\sigma^2$.
- The mean $\mu_i$ is composed of the intercept $\beta_0$ and the sum of a vector of predictor coefficients $\beta_k$.
- The values of the unknown parameters can be estimated from the outcome variable $y_i$ and the vector of predictors $x_{ki}$.


</div>



## Normal linear model {.smaller}

<div style="float: left; width: 35%;">

$$
y_i \sim N(\mu_i, \sigma^2)\\
\mu_i = \beta_0 + \sum_{k=1}^K \beta_k \cdot x_{ki}\\
\text{for } i \in 1\dots n
$$
</div>

<div style="float: right; width: 55%;">

```{r}
# Load and transform data
blomkvist <- read_csv("../data/blomkvist.csv") %>% 
  select(id, sex, age, rt = rt_hand_d) %>% 
  mutate(log_rt = log(rt)) %>% 
  drop_na()
```

```{r}
# Specify model
model <- lm(log_rt ~ sex + age, data = blomkvist)
```

</div>



## Maximum likelihood estimates {.smaller}

The maximum likelihood values of the $K+1$ coefficients, $\hat\beta_0,\hat\beta_1\dots\hat\beta_K$, are those values of the $\beta_0,\beta_1\dots\beta_K$ variables that minimize the *residual sum of squares*

<div style="float: left; width: 45%;">

$$
\text{RSS} = \sum_{i=1}^n \mid y_i - \mu_i\mid^2
$$
</div>

<div style="float: right; width: 45%;">

```{r}
coef(model)
```

</div>

<div style="float: left; width: 100%;">

Once we have $\hat\beta$, then the maximum likelihood estimate of $\sigma$, denoted as $\hat\sigma$ is:

</div>

<div style="float: left; width: 45%;">

$$
\hat\sigma = \sqrt{\frac{1}{n-K-1}\sum_{i=1}^n\mid y_i-\hat\mu_i\mid^2}
$$
</div>

<div style="float: right; width: 45%;">

```{r}
sigma(model)
```

</div>


## Update your workshop repository

- Go to [github.com/jensroes/stats-iii](https://github.com/jensroes/stats-iii)
- Download the repository
- Extract the `model-comparison` folder


## Why do we need model comparion? {.smaller}

<div style="float: left; width: 35%;">

> "All models are wrong, but some are useful"
>
> George E.P. Box

</div>

<div style="float: right; width: 55%;">

- Models are statistical representations of a hypothesis.
- We want to compare hypotheses.
- How useful is my model compared to other alternative models?
- Can my model fit the data (more) accurately?
- Are my model's predictions useful?
- Can the model accurately predict future outcomes?

</div>

## Ockham's razor: the principle of parsimony {.smaller}

<div style="float: left; width: 45%;">

- Is it worth assuming a more complex model?
- Philosopher William of Ockham (1285-1347/49): 

"pluralitas non est ponenda sine necessitate"

- "Plurality should not be posited without necessity."
- Precedence to simplicity: of two competing theories, the simpler explanation of an entity is to be preferred. 
- We don't assume more complex theories, if there is not sufficient evidence.

</div>

<div style="float: right; width: 50%;">
```{r echo=F, out.width="100%"}
include_graphics("../gfx/ockham.jpg")
```

</div>

## Overview

- Likelihood of the data.
- Explained and unexplained variance.
- Fit to data.
- Penalising more complex models.


## Overview {.smaller}


```{r overview, echo = F}
read_csv("overview.csv") %>% 
  kable(caption = "Popular model evaluation techniques") %>% 
  kable_styling(full_width = T, bootstrap_options = c("responsive", "hover", "striped"))
```



```{r eval = FALSE, echo=FALSE}
- Model evaluation: check how good the model fits the data
- Testing whether a predictor explains variance and make the model beter
- Hypothesis testing: testing whether one model is better than a simpler model with otherwise compareable properties.

```





## Probabilistic generative models 

- In any statistical analysis, we assume our data are drawn from some probability distribution.
- This is that we mean by *statistical model*, a *probabilistic generative model* like

$$
y_i \sim \mathcal{N}(\mu_i, \sigma^2), \mu_i = \beta_0 + \beta_1 \cdot x_i
$$


- A model of the *statistical population* could also be described as the true generative model.
- In our analysis, we aim to find a good, or good enough, model of the population.


## Model evaluation

- Are the data compatible with the model?
- Calculate the probability of the data according to the model.
- If the probability of observing the data is higher in model $\mathcal{M}_0$ than in another $\mathcal{M}_1$, which model is more compatible with the data?
- The probability of the data according to the model is the model's *likelihood*.


## Probabilistic model of log rts {.smaller}

<div style="float: left; width: 45%;">

A possible model of log rt is the following

$$
y_i \sim N(\mu, \sigma^2)\\
\text{for } i \in 1\dots n,
$$
where $y_i$ is the log rt of observation $i$.

We are modelling log rt as normally distributed with a mean $\mu$ and standard deviation $\sigma$, but we don't know the value of either of these parameters.

</div>
<div style="float: right; width: 45%;">

```{r echo=F, out.width="80%", fig.height=6}
ggplot(blomkvist, aes(x = log_rt)) +
  geom_histogram()
```

</div>


## Model likelihood {.smaller}

What is the probability of the observed values of the log rt variable, $y_1, y_2, y_3\dots y_n$, assuming values for $\mu$ and $\sigma$ (based on a particular model).


$$
P(y_1\dots y_n\mid\mu, \sigma)
$$

We assume that all $y$'s are conditionally independent of one another, so the joint probability is 

$$
P(y_1\dots y_n\mid\mu, \sigma) = \prod_\text{i=1}^n P(y_i\mid \mu, \sigma).
$$

We don't know the values of $\mu$ and $\sigma$, so we use their *maximum likelihood estimates*: $\hat\mu$ and $\hat\sigma$:

$$
P(y_1\dots y_n\mid\hat\mu, \hat\sigma) = \prod_\text{i=1}^n P(y_i\mid \hat\mu, \hat\sigma).
$$

## Model log likelihood {.smaller}

The join probability (the product of probabilities)

$$
P(y_1\dots y_n\mid\mu, \sigma) = \prod_\text{i=1}^n P(y_i\mid \mu, \sigma).
$$


will be a very small number (called numeric underflow), so we use its logarithm:


$$
\text{log} \left( P(y_1\dots y_n\mid\mu, \sigma) \right) = \sum_\text{i=1}^n \text{log } P(y_i\mid \mu, \sigma),
$$

and with maximum likelihood estimators $\hat\mu, \hat\sigma$ for the unknowns.


## Model log likelihood: calculation

```{r}
model_0 <- lm(log_rt ~ 1, data = blomkvist)
mu_hat <- coef(model_0) # MLE of mu
sigma_hat <- sigma(model_0) # MLE of sigma
```

```{r}
logp <- dnorm(blomkvist$log_rt, mean = mu_hat, sd = sigma_hat, log = TRUE)
sum(logp)
```

```{r}
logLik(model_0)
```

Work through exercise script: `exercises/log_likelihood_1.R`


## Adding a predictor {.smaller}

<div style="float: left; width: 40%;">

```{r}
select(blomkvist, id, log_rt, sex)
```

</div>

<div style="float: right; width: 50%;">
Using the `sex` variable, the potential model of the log rt data is

$$
\begin{aligned}
y_i &\sim N(\mu_i, \sigma^2)\text{ for } i \in 1\dots n,\\
\mu_i&= \beta_0 + \beta_1 \cdot x_i,
\end{aligned}
$$
where $y_i$ is the log rt and $x_i$ is `sex` on observation $i$.

We are modelling log rt as normally distributed as a function of `sex`, and with an equal variance $\sigma^2$.

Unknowns are $\beta_0,\beta_1,\sigma^2$.

## Regression model likelihood {.smaller}

Assuming values for $\beta_0,\beta_1,\sigma$, what is the probability of the observed values of log rt, $y_1, y_2, y_3\dots y_n$? 

$$
P(y_1\dots y_n\mid x_1\dots x_n,\beta_0,\beta_1,\sigma) = \prod_\text{i=1}^n P(y_i\mid x_i,\beta_0,\beta_1,\sigma).
$$

The log likelihood of the model is

$$
\sum_\text{i=1}^n\text{ log} P(y_i\mid x_i,\beta_0,\beta_1,\sigma).
$$

Again, we don't know $\beta_0,\beta_1,\sigma$, so we use their MLEs, $\hat\beta_0,\hat\beta_1,\hat\sigma$.

</div>

## Regression model log likelihood: calculation {.smaller}


```{r}
model_1 <- lm(log_rt ~ sex, data = blomkvist)
mu_hat <- predict(model_1) # mu_hat = beta_0 + beta_1 * x
sigma_hat <- sigma(model_1)
```

```{r}
logp <- dnorm(blomkvist$log_rt, mean = mu_hat, sd = sigma_hat, log = TRUE)
sum(logp)
```

```{r}
logLik(model_1)
```

Work through exercise script: `exercises/log_likelihood_2.R`


## Compare log likelihoods {.smaller}

- Which model are the data more compatible with?

<div style="float: left; width: 45%;">

```{r}
logLik(model_0) # L_0
```

</div>

<div style="float: right; width: 45%;">

```{r}
logLik(model_1) # L_1
```

</div>

<div style="float: left; width: 100%;">

<div style="float: left; width: 45%;">

- Let's denote these log likelihoods as log $\mathcal{L}_0$ and log $\mathcal{L}_1$:

$$
\begin{aligned}
\text{log} \left(\frac{\mathcal{L}_1}{\mathcal{L}_0}\right)&=\text{log}\mathcal{L}_1-\text{log}\mathcal{L}_0\\
&=-13.3--17.6\\
&=4.3
\end{aligned}
$$




</div>

<div style="float: right; width: 45%;">

- Including `sex` appears to increase the log likelihood.
- Work through exercise script: `exercises/compare_log_lik.R`

</div>
</div>


## Residuals: assessing the unexplained variance

<div style="float: left; width: 45%;">

```{r echo = F, out.width="100%"}
set.seed(123)
n <- 20
beta_0 <- 50
beta_1 <- 6
sigma <- 3
sim <- tibble(id = 1:6) %>%
    mutate(x = 0:5,
           e = rnorm(n(), mean = 0, sd = sigma)) %>%
    mutate(y = beta_0 + beta_1 * x + e) %>%
  select(id, x, y) 

fake_model <- lm(y ~ x, data = sim)

ggplot(data =sim, aes(x = x, y = predict(fake_model))) +
  geom_point(aes(y = y), size = 4) +
  geom_line(colour = "red") +
  labs(subtitle = bquote(beta[0] == .(beta_0)*","~beta[1] == .(beta_1)*","~sigma^2 == .(sigma))) +
  theme(axis.title = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank())
```

</div>



<div style="float: right; width: 45%;">

```{r echo = F, out.width="100%"}
set.seed(123)
n <- 20
beta_0 <- 50
beta_1 <- 6
sigma <- 20
sim <- tibble(id = 1:6) %>%
    mutate(x = 0:5,
           e = rnorm(n(), mean = 0, sd = sigma)) %>%
    mutate(y = beta_0 + beta_1 * x + e) %>%
  select(id, x, y) 

fake_model <- lm(y ~ x, data = sim)

ggplot(data = sim, aes(x = x, y = predict(fake_model))) +
  geom_point(aes(y = y), size = 4) +
  geom_line(colour = "red") +
  labs(subtitle = bquote(beta[0] == .(beta_0)*","~beta[1] == .(beta_1)*","~sigma^2 == .(sigma))) +
  theme(axis.title = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank())
```

</div>

<div style="float: left; width: 100%;">

- Which model is better and why?
- How can we reduce the residual error?

</div>


## Residuals: assessing the unexplained variance

<div style="float: left; width: 45%;">

```{r echo = F, out.width="100%"}
set.seed(123)
n <- 20
beta_0 <- 50
beta_1 <- 6
sigma <- 3
sim <- tibble(id = 1:6) %>%
    mutate(x = 0:5,
           e = rnorm(n(), mean = 0, sd = sigma)) %>%
    mutate(y = beta_0 + beta_1 * x + e) %>%
  select(id, x, y) 

fake_model <- lm(y ~ x, data = sim)

ggplot(data =sim, aes(x = x, y = predict(fake_model))) +
  geom_point(aes(y = y), size = 4) +
  geom_line(colour = "red") +
  labs(subtitle = bquote(beta[0] == .(beta_0)*","~beta[1] == .(beta_1)*","~sigma^2 == .(sigma))) +
  geom_segment( aes(xend = x, yend = y),
      size = 0.5, alpha = 0.5, lineend = "round") +
  theme(axis.title = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank())
```

</div>



<div style="float: right; width: 45%;">

```{r echo = F, out.width="100%"}
set.seed(123)
n <- 20
beta_0 <- 50
beta_1 <- 6
sigma <- 20
sim <- tibble(id = 1:6) %>%
    mutate(x = 0:5,
           e = rnorm(n(), mean = 0, sd = sigma)) %>%
    mutate(y = beta_0 + beta_1 * x + e) %>%
  select(id, x, y) 

fake_model <- lm(y ~ x, data = sim)

ggplot(data = sim, aes(x = x, y = predict(fake_model))) +
  geom_point(aes(y = y), size = 4) +
  geom_line(colour = "red") +
  labs(subtitle = bquote(beta[0] == .(beta_0)*","~beta[1] == .(beta_1)*","~sigma^2 == .(sigma))) +
  geom_segment( aes(xend = x, yend = y),
      size = 0.5, alpha = 0.5, lineend = "round") +
  theme(axis.title = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank())
```

</div>

<div style="float: left; width: 100%;">

- Which model is better and why?
- How can we reduce the residual error?

</div>


## Residuals: assessing the unexplained variance


<div style="float: left; width: 45%;">



```{r out.width="100%", fig.height=6.5, echo = F}
blomkvist$pred <- predict(model)
blomkvist$resid <- residuals(model)

plot <- ggplot(blomkvist, aes(x = age, y = pred, colour = sex) ) +
  geom_line(size = 2) +
  scale_colour_viridis_d(end = .6) +
  geom_point(aes(x = age, y = log(rt), colour = sex), size = 2);plot
```
</div>


## Residuals: assessing the unexplained variance {.smaller}

<div style="float: left; width: 45%;">

```{r out.width="100%", fig.height=6.5, echo = F}

plot <- plot + geom_segment(
      aes(xend = age, yend = log(rt)),
      size = 0.5, alpha = 0.5, lineend = "round"
      );plot
```
</div>

<div style="float: right; width: 50%;">
- Residuals are the unexplained (residual) variance: error in the modelling results.
- Distance between observed ($y$) and predicted rt ($\hat{\mu}$)

$$
\epsilon = y - \hat{\mu}
$$

- The closer the residuals are to 0, the lower the prediction error.

</div>


## Residuals: assessing the unexplained variance {.smaller}

<div style="float: left; width: 45%;">

```{r out.width="100%", fig.height=6.5, echo = F}
plot
```
</div>

<div style="float: right; width: 45%;">

```{r}
mutate(blomkvist, 
       predicted = predict(model),
       resid_1 = log_rt - predicted,
       resid_2 = residuals(model)) %>% 
  select(log_rt, pred, resid_1, resid_2)  
```

Work through exercise script: `exercises/residuals.R`


</div>




## RSS: Residual sum of squares {.smaller}

The sum of squared residuals in normal linear models when using the maximum likelihood estimators is

$$
\begin{aligned}
\text{RSS}&=\sum_\text{i=1}^n \mid y_i - (\hat\beta_0+\hat\beta_1\cdot x_i)\mid^2,\\
&=\sum_\text{i=1}^n\mid y_i-\hat{\mu}_i\mid
\end{aligned}
$$

<div style="float: left; width: 45%;">

```{r}
residuals(model)[1:5]
```

</div>

<div style="float: right; width: 45%;">

```{r}
y <- blomkvist$log_rt
mu_hat <- predict(model)
(y - mu_hat)[1:5]
```

</div>


## RSS: Residual sum of squares {.smaller}



```{r}
# RSS: total of unexplained variance
(rss <- sum(residuals(model)^2))
```


```{r echo = F, out.width="100%", fig.height=2.25}
resid <- as.vector(residuals(model))
y <- blomkvist$log_rt
#as.vector(predict(model_0, newdata = blomkvist))

y_plot <- ggplot(data = NULL, aes(x = y)) +
  geom_histogram() +
  labs(x = "y") +
  theme_bw(base_size = 11) +
  theme(axis.title.y = element_blank())

resid_plot <- ggplot(data = NULL, aes(x = resid)) +
  geom_histogram() +
  theme_bw(base_size = 11) +
  labs(x = bquote("residuals" == "y" - tilde("y"))) +
  theme(axis.title.y = element_blank())

resid_sqrt_plot <- ggplot(data = NULL, aes(x = resid^2)) +
  geom_histogram() +
  theme_bw(base_size = 11) +
  labs(x = bquote("residuals"^2)) +
  theme(axis.title.y = element_blank())

y_plot + resid_plot + resid_sqrt_plot
```



## Residual sum of squares and log likelihood {.smaller}

RSS is a measure of the model's lack of fit.

```{r}
(rss <- sum(residuals(model)^2))
```

Log likelihood and RSS are related as follows:

$$
\text{log}(\mathcal{L})=-\frac{n}{2}\left( \text{log}(2\pi)-\text{log}(n) + \text{log(RSS)}+1 \right)
$$
<div style="float: left; width: 45%;">
```{r}
n <- nrow(blomkvist)
-(n/2) * (log(2*pi) - log(n) + log(rss) + 1)
```
</div>

<div style="float: right; width: 45%;">
```{r}
logLik(model)
```
</div>

<div style="float: left; width: 100%;">
The differences in likelihood is determined only by differences in RSS.
</div>

## ESS: Explained sum of squares {.smaller}


<div style="float: left; width: 50%;">

Predicted mean value $\hat\mu_i$ for each observation $i$:

$\mu_i = \beta_0 + \beta_1 \cdot x_{i}$

```{r}
(mu <- predict(model))[1:10]
```

$\bar{y}$ is the mean of the sample.

```{r}
(y_bar <- mean(blomkvist$log_rt))
```

</div>

<div style="float: right; width: 40%;">

ESS is sum of squared differences of predicted data and sample mean.

$$
\text{ESS} = \sum_{i=1}^n(\hat\mu_i-\bar{y})^2
$$

```{r}
# ESS: explained sum of squares
(ess <- sum( (mu - y_bar)^2 ))
```

</div>



## $R^2$: coefficient of determination {.smaller}

Ability of a model to predict the outcome variable.

$$
\underbrace{\sum_{i=1}^n(y_i-\bar{y})^2}_\text{TSS} = \underbrace{\sum_{i=1}^n(\hat\mu_i-\bar{y})^2}_\text{ESS} + \underbrace{\sum_{i=1}^n(y_i-\hat\mu_i)^2}_\text{RSS}
$$

<div style="float: left; width: 100%;">

<div style="float: left; width: 45%;">

- Proportion of total variation explained by the variation in the predictor variable(s).
- Proportion of the variability in the outcome variable due to changes in the predictor(s).
- Routinely used as measure of model fit.

$$
R^2 = \frac{\text{ESS}}{\text{TSS}}
$$

```{r}
(tss <- ess + rss)
```

</div>
<div style="float: right; width: 45%;">


```{r}
ess / tss
```

```{r}
summary(model)$r.sq
```


Work through script: `exercises/rsquared.R`


</div>
</div>


## Root mean square error {.smaller}

<div style="float: left; width: 45%;">

- In contrast to highlighting variance explained.
- The larger the sample, the larger the RSS.
- Alternative to RSS is the square root of the mean squared residuals.
- Summaring average distance between model fit and data, while taking into account sample size.


$$
\text{RMSE} = \sqrt{\frac{\text{RSS}}{n}},
$$

```{r}
n <- nrow(blomkvist)
rss <- sum(residuals(model)^2)
sqrt(rss/n)
```

</div>

<div style="float: right; width: 45%;">


```{r}
sqrt(mean(residuals(model)^2))
```

- Standard deviation of the prediction error.

```{r}
sd(residuals(model))
```


which is $\hat\sigma$

```{r}
sigma(model)
```

</div>


## Mean absolute error

- Related to the root mean squared error is the mean absolute error (MAE), which is the mean of the absolute values of the residuals.

$$
\text{MAE} = \frac{\sum_\text{i=1}^n\mid y_i-\hat{y}_1\mid}{n}
$$

```{r}
sum(abs(residuals(model))) / n
```


```{r}
mean(abs(residuals(model)))
```


Check out exercise `exercises/rmse_mae.R`



## Adjusted $R^2$ {.smaller}

<div style="float: left; width: 45%;">

- The value of $R^2$ necessarily increases, or does not decrease, as we add more predictors to the model, even if the true values of the coefficients for these predictors are zero.
- Overfitting: Complex models explain more variance but may make over optimistic predictions [@gelman2020regression].
- Example: brain size and body mass for seven primate species [from @mcelreath2016statistical].

</div>

<div style="float: right; width: 45%;">

```{r fig.height=6.5, out.width="100%", echo = F}
d <- tibble(species = c("afarensis", "africanus", "habilis", "boisei", "rudolfensis", "ergaster", "sapiens"), 
         brain   = c(438, 452, 612, 521, 752, 871, 1350), 
         mass    = c(37.0, 35.5, 34.5, 41.5, 55.5, 61.0, 53.5))

fit_lm <- function(model, formula){
  model <- lm(data = d, formula = formula)
}

fits <- tibble(model   = str_c("b6.", 1:6),
         formula = c("brain ~ mass", 
                     "brain ~ mass + I(mass^2)", 
                     "brain ~ mass + I(mass^2) + I(mass^3)", 
                     "brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4)", 
                     "brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(mass^5)", 
                     "brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(mass^5) + I(mass^6)")) %>% 
  mutate(fit     = map2(model, formula, fit_lm)) %>% 
  mutate(tidy    = map(fit, tidy),
         glance  = map(fit, glance))

fits <- fits %>% 
  mutate(adjr2      = glance %>% map_dbl("adj.r.squared")) %>% 
  mutate(r2      = glance %>% map_dbl("r.squared")) %>% 
  mutate(r2_text = round(r2, digits = 2) %>% as.character() %>% str_replace(., "0.", "."))

ggplot(d, aes(x = mass, y = brain, label = species)) +
#  geom_point(size = 2.5, shape = 21) +
  geom_text() +
  scale_x_continuous(limits = c(33, 62), expand = c(0, 0)) +
  coord_cartesian(ylim = c(300, 1500)) +
  stat_smooth(method = "lm", fullrange = TRUE, level = .89,
              size = 1/2, alpha = 1/3, colour = "darkred",
              formula = y ~ x) +
  labs(title = "Linear model", subtitle = expression(paste(italic(R)^2, " = .49")),
       y  = "Brain volume (cc)", x = "Body mass (kg)")
```

</div>



## Adjusted $R^2$ 

```{r echo = F, out.width="90%", fig.width=12}
d <- tibble(species = c("afarensis", "africanus", "habilis", "boisei", "rudolfensis", "ergaster", "sapiens"), 
         brain   = c(438, 452, 612, 521, 752, 871, 1350), 
         mass    = c(37.0, 35.5, 34.5, 41.5, 55.5, 61.0, 53.5))

fit_lm <- function(model, formula){
  model <- lm(data = d, formula = formula)
}

fits <- tibble(model   = str_c("b6.", 1:6),
         formula = c("brain ~ mass", 
                     "brain ~ mass + I(mass^2)", 
                     "brain ~ mass + I(mass^2) + I(mass^3)", 
                     "brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4)", 
                     "brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(mass^5)", 
                     "brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(mass^5) + I(mass^6)")) %>% 
  mutate(fit     = map2(model, formula, fit_lm)) %>% 
  mutate(tidy    = map(fit, tidy),
         glance  = map(fit, glance))


fits <- fits %>% 
  mutate(r2      = glance %>% map_dbl("r.squared")) %>% 
  mutate(r2_text = round(r2, digits = 2) %>% as.character() %>% str_replace(., "0.", "."))

p <-  ggplot(d, aes(x = mass, y = brain, label = species)) +
  geom_point(size = 2.5, shape = 21) +
  scale_x_continuous(limits = c(33, 62), expand = c(0, 0)) +
  coord_cartesian(ylim = c(300, 1500)) +
  theme(axis.title = element_blank(),
        plot.title = element_text(size = 14),
        plot.subtitle = element_text(size = 12))

# linear
p1 <-  p + stat_smooth(method = "lm", fullrange = TRUE, level = .89,
              size = 1/2, alpha = 1/3, colour = "darkred",
              formula = y ~ x) +
  labs(title = "Linear model", subtitle = expression(paste(italic(R)^2, " = .49")))
  
# cubic
p3 <- p + stat_smooth(method = "lm", fullrange = TRUE, level = .89,
              size = 1/2, alpha = 1/3,  colour = "darkred",
              formula = y ~ poly(x, 3)) +
  labs(title = "3rd-order polynomial", subtitle = expression(paste(italic(R)^2, " = .68")))

# sixth-order polynomial
p6 <-  p + geom_hline(yintercept = 0, linetype = 2) + 
  stat_smooth(method = "lm", fullrange = TRUE, level = .89,
              size = 1/2, alpha = 1/3,  colour = "darkred",
              formula = y ~ poly(x, 6)) +
  coord_cartesian(ylim = c(-300, 1500)) +
  labs(title = "6th-order polynomial", subtitle = expression(paste(italic(R)^2, " = 1")))

plot <- p1 + p3 + p6

gt <- patchwork::patchworkGrob(plot)
gridExtra::grid.arrange(gt, left = "Brain volume (cc)", bottom = "Body mass (kg)")
```



## Adjusted $R^2$

To overcome this spurious increase in $R^2$, the following adjustment is applied.

$$
\begin{aligned}
R^2_\text{Adj} &= \frac{\text{ESS}}{\text{TSS}} \cdot \frac{n-1}{n-K-1}\\
R^2_\text{Adj} &= R^2 \cdot \underbrace{\frac{n-1}{n-K-1}}_{\text{penalty}}\\
\end{aligned}
$$


## Adjusted $R^2$ 

```{r echo = F, out.width="90%", fig.width=12}
d <- tibble(species = c("afarensis", "africanus", "habilis", "boisei", "rudolfensis", "ergaster", "sapiens"), 
         brain   = c(438, 452, 612, 521, 752, 871, 1350), 
         mass    = c(37.0, 35.5, 34.5, 41.5, 55.5, 61.0, 53.5))

fit_lm <- function(model, formula){
  model <- lm(data = d, formula = formula)
}

fits <- tibble(model   = str_c("b6.", 1:6),
         formula = c("brain ~ mass", 
                     "brain ~ mass + I(mass^2)", 
                     "brain ~ mass + I(mass^2) + I(mass^3)", 
                     "brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4)", 
                     "brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(mass^5)", 
                     "brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(mass^5) + I(mass^6)")) %>% 
  mutate(fit     = map2(model, formula, fit_lm)) %>% 
  mutate(tidy    = map(fit, tidy),
         glance  = map(fit, glance)) %>% 
  mutate(r2      = glance %>% map_dbl("adj.r.squared"))  

p <-  ggplot(d, aes(x = mass, y = brain, label = species)) +
  geom_point(size = 2.5, shape = 21) +
  scale_x_continuous(limits = c(33, 62), expand = c(0, 0)) +
  coord_cartesian(ylim = c(300, 1500)) +
  theme(axis.title = element_blank(),
        plot.title = element_text(size = 14),
        plot.subtitle = element_text(size = 12))

# linear
p1 <-  p + stat_smooth(method = "lm", fullrange = TRUE, level = .89,
              size = 1/2, alpha = 1/3, colour = "darkred",
              formula = y ~ x) +
  labs(title = "Linear model", subtitle = expression(paste(italic(R)["Adj"]^2, " = .39")))
  
# cubic
p3 <- p + stat_smooth(method = "lm", fullrange = TRUE, level = .89,
              size = 1/2, alpha = 1/3,  colour = "darkred",
              formula = y ~ poly(x, 3)) +
  labs(title = "3rd-order polynomial", subtitle = expression(paste(italic(R)["Adj"]^2, " = .36")))

# sixth-order polynomial
p6 <-  p + geom_hline(yintercept = 0, linetype = 2) + 
  stat_smooth(method = "lm", fullrange = TRUE, level = .89,
              size = 1/2, alpha = 1/3,  colour = "darkred",
              formula = y ~ poly(x, 6)) +
  coord_cartesian(ylim = c(-300, 1500)) +
  labs(title = "6th-order polynomial", subtitle = expression(paste(italic(R)["Adj"]^2, " = NaN")))

plot <- p1 + p3 + p6

gt <- patchwork::patchworkGrob(plot)
gridExtra::grid.arrange(gt, left = "Brain volume (cc)", bottom = "Body mass (kg)")
```




## $R^2$ and Adjusted $R^2$ with `lm` {.smaller}

```{r}
# Specify models
model_0 <- lm(log_rt ~ 1, data = blomkvist)
model_1 <- lm(log_rt ~ sex, data = blomkvist)
# Calculate residual sum of squares
RSS_0 <- sum(residuals(model_0)^2)
RSS_1 <- sum(residuals(model_1)^2)
# Compare
c(RSS_0, RSS_1)
```

```{r}
(RSS_0 - RSS_1)/RSS_1
```


## $R^2$ and Adjusted $R^2$ with `lm` {.smaller}


<div style="float: left; width: 55%;">

```{r}
# Specify models
model_0 <- lm(log_rt ~ 1, data = blomkvist)
model_1 <- lm(log_rt ~ sex, data = blomkvist)
```

From the `lm` objects, $R^2$ and $R_\text{Adj}^2$ can be obtained using `summary`

```{r}
# Generate model summary
summary_model_0 <- summary(model_0)
summary_model_1 <- summary(model_1)
```

</div>

<div style="float: right; width: 35%;">

```{r}
summary_model_0$r.squared
```

```{r}
summary_model_1$r.squared
```

```{r}
summary_model_0$adj.r.squared
```

```{r}
summary_model_1$adj.r.squared
```

</div>


## Calculating $R^2$ and $R_\text{Adj}^2$ {.smaller}
 
<div style="float: left; width: 45%;">

```{r}
y <- blomkvist %>% pull(log_rt)
mu <- predict(model_1)
tss <- sum( (y - mean(y) )^2)
ess <- sum( (mu - mean(y))^2)
rss <- sum( residuals(model_1)^2)
(rsq <- ess/tss) # R^2
```

```{r}
summary_model_1$r.squared
```


```{r}
summary_model_1$adj.r.squared
```

</div>

<div style="float: right; width: 45%;">

```{r}
n <- length(y)
K <- 1 # number of predictors
(adj_rsq <- rsq * (n-1)/(n-K-1))
```


Because $n \gg K$, the penalty term is (very) close to 1.0 and the adjustment is minimal.

```{r}
(n - 1) / (n - K - 1)
```


Do exercise `exercises/adjrsquared.R`


</div>



## Model comparison {.smaller}

- $R^2=0$ is only true if $\beta_1=\beta_2=\dots\beta_k=0$.
- When all coefficients are simultaneously zero, we are essentially saying that the following two models are identical.

$$
\begin{aligned}
\mathcal{M}_0:& y_i \sim N(\hat\mu, \sigma^2), \hat\mu_i = \beta_0, \text{ for } i \in 1\dots n,\\
\mathcal{M}_1:& y_i \sim N(\hat\mu, \sigma^2), \hat\mu_i = \beta_0 + \sum_{k=1}^K \beta_k \cdot x_{ki}, \text{ for } i \in 1\dots n,
\end{aligned}
$$


## Model comparison {.smaller}

- The residual sums of squares for these two models can be denoted RSS$_0$ and RSS$_1$.
- Under the null hypothesis that these two models are identical, and so the coefficients for all predictors are simultaneously zero, we have the following result:


$$
\underbrace{\frac{(\text{RSS}_0-\text{RSS}_1)/K}{\text{RSS}_1/(n - K - 1)}}_\text{F-statistic} \sim F(K, n-K-1)
$$

- Under the null hypothesis, the *F*-statistic is distributed as an *F*-distribution with $K$ and $N-K-1$ degrees of freedom.


## Model comparison {.smaller}

- We can extend the above result to test whether any subset of the $K$ predictors have coefficients that are simultaneously zero.
- In general, we can compare two models $\mathcal{M}_1$ and $\mathcal{M}_0$ with $K_1$ and $K_0$ predictors, respectively, and where $K_0 < K$ and all the $K_0$ predictors in $\mathcal{M}_0$ are also presented in $\mathcal{M}_1$ 
- I.e. $\mathcal{M}_0$ is *nested* in $\mathcal{M}_1$.
- Under the null hypothesis that the $K_1-K_0$ predictors in $\mathcal{M}_1$ and not in $\mathcal{M}_0$ are simultanously zero, we have


$$
\begin{aligned}
\text{proportional increase in error} &= \frac{\text{change in error (from } \mathcal{M}_0\text{ to }\mathcal{M}_1)}{\text{minimal error}}\\
&= \frac{\text{RSS}_0-\text{RSS}_1}{\text{RSS}_1}
\end{aligned}
$$

$$
\frac{(\text{RSS}_0-\text{RSS}_1)/(K_1-K_0)}{\text{RSS}_1/(n - K_1 - 1)} \sim F(K_1 - K_0, n-K_1-1).
$$

- Nested normal linear models can be compared using F test.



## Model comparison {.smaller}

- The results of the null hypothesis test that $R^2=0$ can be obtained in numerous ways, bu the easiest is to use the generic `anova` function where we compare `model_1` against `model_0`.  

<div style="float: left; width: 47%;">

```{r}
# Specify models
model_0 <- lm(log_rt ~ 1, data = blomkvist)
model_1 <- lm(log_rt ~ sex, data = blomkvist)
```

```{r eval = F}
# Compare models
anova(model_0, model_1)
```

```{r echo = F}
# Compare models
print(anova(model_0, model_1), signif.stars=F)
```


</div>

<div style="float: right; width: 45%;">

- `RSS` column are TSS and RSS of model `model_1`.
- TSS is RSS of `model_0` because in model with no predictors the variance of the outcome variable and the variance of the residuals are necessarily identical. 
- ESS is the `Sum of Sq` and the difference of the `RSS` values.

```{r}
(ess <- tss - rss)
```


</div>


## Model comparison {.smaller}

- The results of the null hypothesis test that $R^2=0$ can be obtained in numerous ways, bu the easiest is to use the generic `anova` function where we compare `model_1` against `model_0`.  

<div style="float: left; width: 47%;">

```{r}
# Specify models
model_0 <- lm(log_rt ~ 1, data = blomkvist)
model_1 <- lm(log_rt ~ sex, data = blomkvist)
```

```{r eval = F}
# Compare models
anova(model_0, model_1)
```

```{r echo = F}
# Compare models
print(anova(model_0, model_1), signif.stars=F)
```


</div>

<div style="float: right; width: 45%;">


- `Df` and bottom value of `Res.Df` give us the degrees of freedom 

```{r}
c(K, n - K - 1)
```

by which `ess` and `rss` are divided for F:

```{r}
(f_stat <- (ess/K) / (rss/(n - K - 1)))
```

</div>


## Model comparison {.smaller}

- The results of the null hypothesis test that $R^2=0$ can be obtained in numerous ways, bu the easiest is to use the generic `anova` function where we compare `model_1` against `model_0`.  

<div style="float: left; width: 47%;">

```{r}
# Specify models
model_0 <- lm(log_rt ~ 1, data = blomkvist)
model_1 <- lm(log_rt ~ sex, data = blomkvist)
```

```{r eval = F}
# Compare models
anova(model_0, model_1)
```

```{r echo = F}
# Compare models
print(anova(model_0, model_1), signif.stars=F)
```


</div>

<div style="float: right; width: 45%;">

- *p*-value gives the probability of getting a results greater than this F statistic in an F distribution with $K$ and $n - K - 1$ degrees of freedom.
- Cumulative distribution function of the F distribution `pf`

```{r}
pf(f_stat, K, n-K-1, lower.tail = FALSE)
```

</div>


## F-ratio  {.smaller}

$$
\text{F} = \underbrace{\frac{\text{RSS}_0 - \text{RSS}_1}{\text{RSS}_1}}_\text{effect size} \cdot \underbrace{\frac{\text{df}_1}{\text{df}_0 -\text{df}_1}}_\text{sample size} = \frac{(\text{RSS}_0 - \text{RSS}_1)/(\text{df}_0 - \text{df}_1)}{\text{RSS}_1/\text{df}_1}
$$

where Df$_1$ is $n - K_1 + 1$, where $K_1$ is the number of coefficients (predictors) in $\mathcal{M}_1$.

<div style="float: left; width: 45%;">

```{r}
# Dfs
df_0 <- model_0$df.residual
df_1 <- model_1$df.residual
df_0 - df_1; df_1
```

```{r}
# RSS
rss_0 <- sum(residuals(model_0)^2)
rss_1 <- sum(residuals(model_1)^2)
```

</div>

<div style="float: right; width: 45%;">

```{r}
(rss_0 - rss_1)/(df_0 - df_1)
```

```{r echo = F, eval = F}
rss_1 / df_1
```

```{r}
((rss_0 - rss_1)/(df_0 - df_1))/(rss_1/df_1)
```

Complete exercise `exercises/modelcomparison.R`

</div>

## F distribution

```{r}
df1 <- K # Number of fixed effects
df2 <- n - K - 1  
F_grid <- seq(0, 10, .1) 
p_values <- pf(q = F_grid, df1 = df1, df2 = df2, lower.tail = F)
```

```{r echo = F, fig.height=3}
p <- ggplot(data = NULL, aes(x = F_grid, y = p_values)) +
  geom_line() +
  labs(x = "Theoretical F value",
       y = "p value",
       subtitle = bquote("F distribution for"~"df"[1] == .(df1)~"and df"[2] == .(df2)));p 
```


## F distribution

```{r}
df1 <- K # Number of fixed effects
df2 <- n - K - 1  
F_grid <- seq(0, 10, .1) 
p_values <- pf(q = F_grid, df1 = df1, df2 = df2, lower.tail = F)
```

```{r echo = F, fig.height=3}
label <- deparse(bquote(paste("F" == .(round(f_stat, 1)))))

p2 <- p + geom_segment(aes(x = 8.2, y = .2, xend = f_stat, yend = 0.02),
                  arrow = arrow(length = unit(0.35, "cm")),
               colour = "red") +
  geom_label(aes(x = 8, y = 0.3, label = label), parse = T); p2
```

## F distribution

```{r}
df1 <- K # Number of fixed effects
df2 <- n - K - 1  
F_grid <- seq(0, 10, .1) 
p_values <- pf(q = F_grid, df1 = df1, df2 = df2, lower.tail = F)
```

```{r echo = F, fig.height=3}
p_value <- pf(q = f_stat, df1 = df1, df2 = df2, lower.tail = F)
label_2 <- deparse(bquote(paste("p" == .(round(p_value, 3)))))

p3 <- p2 + geom_hline(yintercept = p_value, colour = "red", linetype = "dashed") +
  geom_label(aes(x = .65, y = 0.1, label = label_2), parse = T);p3
```


## F distribution

```{r}
df1 <- K # Number of fixed effects
df2 <- n - K - 1  
F_grid <- seq(0, 10, .1) 
p_values <- pf(q = F_grid, df1 = df1, df2 = df2, lower.tail = F)
```

```{r echo = F, fig.height=3}
p3 + annotate("label", x = 8, y = .8, label = "Work through script\n exercise/f_distribution.R")

```



## F distribution for varying Df


<div style="float: left; width: 45%;">
```{r}
df1 <- seq(1, 21, 3)
df2 <- 300  
F_grid <- seq(0, 5, .1) 
```

```{r echo = F, out.width="100%"}
# Increase df1 as for more complex model
data_df1 <- map_dfr(df1, ~pf(q = F_grid, df1 = .x, df2 = df2, lower.tail = F) %>% tibble(F_grid = F_grid, p = .) %>%  
    mutate(df1 = .x)) 

ggplot(data = data_df1, aes(x = F_grid, y = p, colour = factor(df1))) +
  geom_line() +
  labs(x = "Theoretical F value",
       y = "p value",
       colour = bquote("df"[1])) +
  scale_colour_viridis_d() +
  geom_hline(yintercept = 0.05, linetype = "dotted")

```

</div>

<div style="float: right; width: 45%;">

```{r}
df1 <- 5
df2 <- seq(3, 300, 30)
F_grid <- seq(0, 5, .1) 
```

```{r echo = F, out.width="100%"}
# Increase df1 as for more complex model
data_df2 <- map_dfr(df2, ~pf(q = F_grid, df1 = df1, df2 = .x, lower.tail = F) %>% tibble(F_grid = F_grid, p = .) %>%  
    mutate(df2 = .x)) 

ggplot(data = data_df2, aes(x = F_grid, y = p, colour = factor(df2))) +
  geom_line() +
  labs(x = "Theoretical F value",
       y = "p value",
       colour = bquote("df"[2])) +
  scale_colour_viridis_d() +
  geom_hline(yintercept = 0.05, linetype = "dotted")

```

</div>



## Deviance information criterion (DIC)

$$
\text{DIC} = -2 \cdot \text{log} \hat{\mathcal{L}}
$$

where $\hat{\mathcal{L}}$ is the likelihood of the model using the MLEs.

- *Lower* deviance indicates better model fit
- Equivalent to RSS for generalised linear models.

```{r}
deviance(model_1)
```


```{r}
rss_1
```



## Akaike and Bayesian Information Criterion {.smaller}

<div style="float: left; width: 55%;">

- Estimators of prediction error.
- Relative quality of statistical models for given data set.
- Similar to $R^2$, log likelihood will always increase when adding predictors, even if the true value of the coefficient is zero.
- Prevents overfitting by penalising models with more parameters.
- Trade-off model fit (measured by $\hat{\mathcal{L}}$) and the number of parameters in the model.


$$
\text{AIC} = 2 \cdot K - 2 \cdot \text{log}(\hat{\mathcal{L}})\\
\text{BIC} = \text{log}(n) \cdot K - 2\cdot \text{log}(\hat{\mathcal{L}})\\
$$

where $K$ is the number of model parameters.

</div>
<div style="float: right; width: 35%;">

```{r}
ll <- as.vector(logLik(model_1))
K <- 2 # beta_0, sigma^2
(2 * K) - 2 * ll
```

```{r}
AIC(model_1)
```

```{r}
n <- nrow(blomkvist) 
log(n) * K - 2 * ll
```

```{r}
BIC(model_1)
```

</div>


```{r eval = F, echo=F}
The AIC is designed to choose models with
better predictive ability, thus it tends to favour bigger models as the sample size increases.
BIC is an approximation to Bayesian model comparison by Bayes factors, and prefers models
with higher posterior probability under an implicit weak prior (with an amount of information
equivalent to one observation, see Kass and Wasserman 1995
 If there is a “true” model, the
BIC will tend to select it as the sample size increases. In many situations there may not be a
true model, and collecting more data will uncover more complexity in the process generating
the data, in which case AIC may be more suitable. 
```


## Comparing multiple nested models {.smaller}

What about the varying-intercepts and the varying-intercepts and varying-slopes model?

<div style="float: left; width: 52%;">

```{r}
# Specify models
model_0 <- lm(log_rt ~ 1, data = blomkvist)
model_1 <- lm(log_rt ~ sex, data = blomkvist)
model_2 <- lm(log_rt ~ sex + age, data = blomkvist)
model_3 <- lm(log_rt ~ sex * age, data = blomkvist)
```

</div>

<div style="float: right; width: 43%;">

```{r eval = F}
# Compare models
anova(model_0, model_1, model_2, model_3)
```

```{r echo = F}
# Compare models
print(anova(model_0, model_1, model_2, model_3), signif.stars=F)
```
</div>


## Model evaluation made easy {.smaller}


```{r eval = F}
# Drop highest level predictor 
drop1(model_3, scope = ~ sex * age, test = 'F')
```

```{r echo = F}
# Compare models
print(drop1(model_3, scope = ~ sex * age, test = 'F'), signif.stars=F)
```

```{r}
# All you ever need :)
library(broom)
#library(broom.mixed) # for multilevel models
```


```{r}
tidy(model_3)
```

```{r}
glance(model_3) %>% glimpse()
```

```{r}
augment(model_3)
```

Check `exercises/comparing_multiple_nested_models.R`


## Watch out

- Rejecting the null hypothesis in favour of an alternative model does not mean this model is the best (i.e. it is better than the baseline model).
- Failing to reject the null hypothesis in favour of an alternative model does not mean that the null model is correct or that an additional coefficient is not theoretically relevant.



## The end

- Comparing models with different predictors: nested models.
- Evaluation is based one likelihood of data under the model.
- Penalising more complex models by considering number of parameters.
- Other comparison techniques for models that are not nested within each other.
- Cross-validation to prevent overfitting (e.g. "LOO": Leave-one-out CV, k-fold CV, Monte Carlo CV).
- Models with different probability models (e.g. Poisson vs negative binomial).



```{r eval=FALSE, echo=FALSE}

https://cran.r-project.org/doc/contrib/Faraway-PRA.pdf
Chapter 3 Faraway 
```


## References {.smaller}

<style>
slides > slide { overflow: scroll; }
slides > slide:not(.nobackground):after {
  content: '';
}
</style>


